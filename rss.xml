<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[trycatch.dev]]></title><description><![CDATA[Code, musings, and tea. Lots of tea.]]></description><link>https://trycatch.dev/</link><image><url>https://trycatch.dev/favicon.png</url><title>trycatch.dev</title><link>https://trycatch.dev/</link></image><generator>Ghost 4.2</generator><lastBuildDate>Tue, 11 May 2021 23:41:36 GMT</lastBuildDate><atom:link href="https://trycatch.dev/rss.xml" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Meteorological meanderings, part 4: creating interactive maps]]></title><description><![CDATA[<p>As with part 3, this is not exactly meteorology, but rather a necessary component of a modern meteorological application. The ability to view a map, pan and zoom in it, and click on locations within the map is something we take for granted today, with sites and apps like Google</p>]]></description><link>https://trycatch.dev/blog/meteorological-meanderings-part-4-creating-interactive-maps/</link><guid isPermaLink="false">60935027ab57ad0001c2907b</guid><category><![CDATA[Weather]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Tue, 11 May 2021 23:41:32 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1504807959081-3dafd3871909?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDg2fHxtYXB8ZW58MHx8fHwxNjIwMjY5ODIy&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1504807959081-3dafd3871909?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDg2fHxtYXB8ZW58MHx8fHwxNjIwMjY5ODIy&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Meteorological meanderings, part 4: creating interactive maps"><p>As with part 3, this is not exactly meteorology, but rather a necessary component of a modern meteorological application. The ability to view a map, pan and zoom in it, and click on locations within the map is something we take for granted today, with sites and apps like Google Maps having popularized the concepts. Bringing this functionality to a weather application is only natural.</p><h3 id="map-tiles-and-pyramids">Map tiles and pyramids</h3><p>Have you ever noticed, right after zooming in on an interactive map, that things sometimes look blurry for a moment before clearing up? Or, more jarringly, that occasionally there are gray squares where pieces of a map should be? This is because of the way maps are generated and loaded.</p><p>In order to optimize loading times and minimize data transfer, square tiles of map data are lazy-loaded whenever we access a particular area on a map. These tiles are most often 256x256 pixels, and they contain a &quot;good enough&quot; representation of the desired area at the current zoom level. This way, a minimal number of tiles needs to be loaded in order to show us what we want to see. When we pan to another area, the tiles that haven&apos;t yet been loaded are fetched from the server and displayed. When we zoom, fresh tiles at the new zoom level are loaded and displayed. In order to minimize the jarring effect of clearing the screen and then having all new tiles load when we zoom in, the previous zoom level&apos;s tiles are retained, but at a higher zoom level (which is why they look blurry), and then gradually replaced by tiles at the proper zoom level as they load.</p><p>This concept is often represented as a pyramid:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2021/05/image.png" class="kg-image" alt="Meteorological meanderings, part 4: creating interactive maps" loading="lazy" width="1475" height="1286" srcset="https://trycatch.dev/content/images/size/w600/2021/05/image.png 600w, https://trycatch.dev/content/images/size/w1000/2021/05/image.png 1000w, https://trycatch.dev/content/images/2021/05/image.png 1475w" sizes="(min-width: 720px) 720px"><figcaption>Source: https://www.intechopen.com/books/cartography-a-tool-for-spatial-analysis/web-map-tile-services-for-spatial-data-infrastructures-management-and-optimization</figcaption></figure><p>Pyramided tiles make up the majority of the UI in services like Google Maps. In fact, often there are multiple sets of pyramided tiles layered on top of each other: for example, we can enable an overlay of current traffic conditions &quot;above&quot; the base map. These layers allow for rich customization of displayed data, and it just so happens that layering can be very useful for displaying weather information.</p><p>Overlaying actual weather visualizations is beyond the scope of this post, but we&apos;ll get a good start by creating two map layers: a &quot;bottom&quot; layer with some base information and a &quot;top&quot; layer with geopolitical division lines. The reason for this is, when showing particularly turbulent (and, therefore, colorful) weather, it&apos;s helpful to see where it&apos;s occurring. If we only have one map layer and one weather layer, the weather layer may obscure too much of the map layer, making it difficult to orient ourselves. Having the weather layer sandwiched between bottom and top map layers helps to both see the weather activity clearly and still be able to determine its location relative to known positions without much trouble.</p><h3 id="generating-map-tiles-from-mapping-data">Generating map tiles from mapping data</h3><p>OpenStreetMap data was used for geocoding in part 3, and we&apos;re going to use it again for map tile generation. This time, however, we need to apply different filters to the data. In addition to keeping the names of administrative boundaries and such, we need to see things like rivers, major motorways, and other recognizable landmarks to assist users in orienting themselves. Here is an example of what an <code>osmfilter</code> command might look like for this purpose:</p><pre><code class="language-sh">$ osmfilter source-data.o5m --drop-version --keep=&quot;highway= place= boundary= leisure=park =nature_preserve landuse=recreation_ground natural=water protect_class=&quot; --out-o5m -o=filtered-data.o5m</code></pre><p>At this point, it&apos;d be good to take a step back and note that the steps below are rather manual. This is because they serve as a good explainer of how map tiles work, and they are suitable for certain customization that we&apos;re looking to do. There are, however, easier and quicker ways to generate map tiles. For information on that, take a look at <a href="https://openmaptiles.org/docs/generate/generate-openmaptiles/">OpenMapTiles</a>.</p><p>Instead of letting the Nominatim Docker application take care of importing our data into the database, we&apos;ll do that ourselves using <code>osm2pgsql</code>, which can be installed via <code>apt</code> in <a href="https://packages.debian.org/sid/osm2pgsql">Debian- and Ubuntu-based distributions</a>. The import process looks like this:</p><pre><code class="language-sh">$ createdb -E UTF8 gis
$ psql -c &quot;CREATE EXTENSION postgis;&quot; -d gis
$ psql -c &quot;CREATE EXTENSION hstore;&quot; -d gis
$ osm2pgsql --slim -d gis --hstore --multi-geometry --number-processes 8 -C 2000 filtered-data.o5m</code></pre><p>The above script creates a new database named <code>gis</code>, enables the <code>postgis</code> and <code>hstore</code> Postgres extensions (which, of course, requires <a href="https://postgis.net/">PostGIS</a> to be installed), and finally runs <code>osm2pgsql</code> using our filtered dataset.</p><p>To generate the map tiles, we&apos;ll need a few things. First is a map style definition, which controls how all of the various map elements are rendered. A good starting point is <a href="https://github.com/hotosm/HDM-CartoCSS/">HDM-CartoCSS</a>. It&apos;s a relatively minimal map style, which makes it easy to modify to remove things we don&apos;t need for either the bottom or top layer.</p><p>The second thing we&apos;ll need is a way to compile the map style into a format that can be used for map tile generation. In this case, we&apos;ll use <a href="https://github.com/mapbox/carto">CartoCSS</a>. It&apos;s easy to install using <code>npm</code>, as described in their documentation. CartoCSS takes the YAML (or <code>.mml</code>) file from a style definition like HDM-CartoCSS and outputs a Mapnik XML file, which can then be consumed by tile generation tools compatible with this format.</p><p>The third and final thing we&apos;ll be using here is the actual map tile generator. There are a number of options out there. A pretty simple one is <a href="https://github.com/Zverik/polytiles">PolyTiles</a>, which consists of a single Python file.</p><p>The HDM-CartoCSS wiki specifies some prerequisites that may be useful to download. Additionally, the <code>project.yml</code> file should be modified to point to our <code>gis</code> database. And, of course, we&apos;ll want to make some design changes to the map styles. After that, the rest of the process is straightforward:</p><pre><code class="language-sh">$ cd HDM-CartoCSS
$ mv project.yml project.mml # CartoCSS sometimes complains about file extensions
$ carto project.mml &gt; style.xml
$ cd ..
$ python3 polytiles.py -b -180 -90 180 90 -s HDM-CartoCSS/style.xml -t output --threads 12 -z 0 5</code></pre><p>The above example generates map tiles using 12 threads at zoom levels 0-5. When combining all of the concepts in this post, we can get a bottom layer that might look like this:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/05/image-1.png" class="kg-image" alt="Meteorological meanderings, part 4: creating interactive maps" loading="lazy" width="677" height="653" srcset="https://trycatch.dev/content/images/size/w600/2021/05/image-1.png 600w, https://trycatch.dev/content/images/2021/05/image-1.png 677w"></figure><p>And a top layer that might look like this:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/05/image-2.png" class="kg-image" alt="Meteorological meanderings, part 4: creating interactive maps" loading="lazy" width="671" height="637" srcset="https://trycatch.dev/content/images/size/w600/2021/05/image-2.png 600w, https://trycatch.dev/content/images/2021/05/image-2.png 671w"></figure><p>Combined, the view is pretty reasonable:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/05/image-3.png" class="kg-image" alt="Meteorological meanderings, part 4: creating interactive maps" loading="lazy" width="714" height="641" srcset="https://trycatch.dev/content/images/size/w600/2021/05/image-3.png 600w, https://trycatch.dev/content/images/2021/05/image-3.png 714w"></figure><p>So, if a weather layer was between the two map layers, we would have the ability to see the severity of the weather as well as the major roads and labels on top of it. For less severe weather, where there would be fewer artifacts on the map, the bottom layer would be visible as well, providing additional context.</p><p>To render the map tiles, we can use <a href="https://github.com/Leaflet/Leaflet">Leaflet</a> on a pretty simple HTML page:</p><pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;utf-8&quot; /&gt;
        &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
        &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/leaflet@1.3.1/dist/leaflet.css&quot; integrity=&quot;sha512-Rksm5RenBEKSKFjgI3a41vrjkw4EVPlJ3+OiI65vTjIdo9brlAacEuKOiQ5OFh7cOI1bkDwLqdLw3Zg0cRJAAQ==&quot; crossorigin=&quot;&quot;/&gt;
        &lt;script src=&quot;https://unpkg.com/leaflet@1.3.1/dist/leaflet.js&quot; integrity=&quot;sha512-/Nsx9X4HebavoBvEBuyp3I7od5tA0UzAxs+j83KgC8PU0kgB4XiK4Lfe4y4cgBtaRJQEIFCW+oC506aPT2L1zw==&quot; crossorigin=&quot;&quot;&gt;&lt;/script&gt;
        &lt;style&gt;
            html, body, #map {
                width: 100%;
                height: 100%;
                margin: 0;
                padding: 0;
            }
        &lt;/style&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;div id=&quot;map&quot;&gt;&lt;/div&gt;
        &lt;script&gt;
            var map = L.map(&apos;map&apos;).setView([0, 0], 3);
            L.tileLayer(&apos;output/{z}/{x}/{y}.png&apos;, {
                minZoom: 0,
                maxNativeZoom: 5,
                maxZoom: 8,
                attribution: &apos;Map data &amp;copy; &lt;a href=&quot;https://www.openstreetmap.org/&quot;&gt;OpenStreetMap&lt;/a&gt; contributors, &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/2.0/&quot;&gt;CC-BY-SA&lt;/a&gt;&apos;,
                id: &apos;base&apos;
            }).addTo(map);
        &lt;/script&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>The call to <code>L.tileLayer</code> above is the most important part: it defines how our tiles are structured in the filesystem, which zoom levels are available, and controls other aspects of displaying them. Leaflet then takes care of loading the appropriate tiles, combining them into a cohesive view, and allowing users to pan and zoom around the map.</p><p>This was an overview of creating and displaying customized map tiles. As mentioned above, there are other, more automated, ways of doing this as well. A lot of online resources exist for various cartographic applications &#x2013; more than for meteorology, it would seem. I&apos;m not sure if there will be a part 5 of this blog post series, but if there is, it will likely go into detail on the integration of mapping and weather data.</p>]]></content:encoded></item><item><title><![CDATA[Switching from KeePass to Bitwarden after 14 years]]></title><description><![CDATA[<p>Taking a little break from my <em>Meteorological meanderings</em> posts, I wanted to share my experience in switching away from <a href="https://keepass.info/">KeePass</a>, which I have used since at least 2007, to <a href="https://bitwarden.com/">Bitwarden</a>, a much more modern open source password manager. I&apos;ve been a fan of the idea of password managers</p>]]></description><link>https://trycatch.dev/blog/switching-from-keepass-to-bitwarden-after-14-years/</link><guid isPermaLink="false">603c156e496a4900019b4067</guid><category><![CDATA[Self-hosting]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Sat, 06 Mar 2021 00:58:35 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1609358905581-e5381612486e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDN8fGJhbmslMjB2YXVsdHxlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1609358905581-e5381612486e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDN8fGJhbmslMjB2YXVsdHxlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Switching from KeePass to Bitwarden after 14 years"><p>Taking a little break from my <em>Meteorological meanderings</em> posts, I wanted to share my experience in switching away from <a href="https://keepass.info/">KeePass</a>, which I have used since at least 2007, to <a href="https://bitwarden.com/">Bitwarden</a>, a much more modern open source password manager. I&apos;ve been a fan of the idea of password managers for a long time. They encourage the use of complex passwords that you don&apos;t have to remember and that attackers are unlikely to brute-force; they help to make <a href="https://en.wikipedia.org/wiki/Credential_stuffing">credential stuffing</a> attacks effectively obsolete; and, these days, they speed up the login processes to various websites using browser extensions and mobile apps.</p><p>That said, I&apos;ve been stuck in the past for a while now. While I have no concerns with the security of KeePass itself, the problem is, in order to have the more convenient features like automatic syncing, browser integration, and cross-platform compatibility, I&apos;d have to use various third party tools, forks, implementations, or plugins. That has always made me just uncomfortable enough to avoid. Of course, I couldn&apos;t realistically avoid things like automatic syncing of the password database, so I came up with my own solution of using Box.com to sync the .kdbx file across my devices, while using an offline <a href="https://play.google.com/store/apps/details?id=keepass2android.keepass2android_nonet">KeePass-compatible Android app</a>. My reasoning is that, since the KeePass applications function offline and the syncing applications have no concept of what it is they&apos;re syncing, it should be safe enough to avoid potentially unpleasant situations like the sync service knowing a little too much about the password database, or the password manager syncing to something I didn&apos;t authorize. (Yes, these are mostly theoretical concerns, but I like to err on the side of paranoia when it comes to my passwords.)</p><p>I&apos;ve been aware of Bitwarden for some time, but I&apos;ve shied away from seriously considering it because it didn&apos;t &#x2013; and still doesn&apos;t &#x2013; support modifying password information while offline. I&apos;ve since reconsidered my stance on that and decided that it&apos;s not important enough of a feature to be a deal breaker for me. And besides, it does support offline read access to cached vault data, which should be good enough. I&apos;m still not comfortable letting an informed third party store my password data, which is why I decided to host an instance of <a href="https://github.com/dani-garcia/bitwarden_rs">Bitwarden_RS</a> at home instead of using Bitwarden&apos;s own cloud hosting service. Bitwarden_RS is a lean implementation of the Bitwarden server, written in Rust. Since all Bitwarden clients have the ability to communicate with self-hosted servers (which, in my book, makes Bitwarden by far the number one modern password manager), I&apos;m able to use the official apps mostly as intended by the developer, while keeping the data as private as it can reasonably get.</p><p>Since I didn&apos;t want to expose my Bitwarden_RS instance to the public internet, and I didn&apos;t want to deal with the hassle of VPN connections interfering with routing as they often tend to, I decided to put the server as well as all the clients (including my phone) on a <a href="https://www.zerotier.com/">ZeroTier</a> network dedicated to Bitwarden. ZeroTier creates virtual, decentralized software defined networks. I tend to think of it as a lightweight hybrid between a VPN and a LAN connection. In practice, I can create, for example, a 192.168.128.0/24 network and authorize any of my devices to access this network and the other devices on it, as long as they have the ZeroTier One client installed. I haven&apos;t experienced any routing trouble with this setup, unlike traditional VPNs. Adding to this mix some public and private DNS entries pointing to an IP in a <a href="https://en.wikipedia.org/wiki/Private_network">non-routable address space</a> and a <a href="https://letsencrypt.org/">Let&apos;s Encrypt</a> certificate for those DNS entries, and the end result is a valid HTTPS endpoint that is reachable only on the private ZeroTier network (and my LAN). Unfortunately, I have run into an issue with (surprise!) a VPN connection interfering with ZeroTier, but it&apos;s something that I hope will be resolved soon with a configuration change to the VPN.</p><p>Migrating my data from KeePass to Bitwarden was very easy: export the unencrypted KeePass XML file, import it into the Bitwarden vault, and then <a href="https://docs.microsoft.com/en-us/sysinternals/downloads/sdelete">sdelete</a> the XML file. The only issue I encountered was that Bitwarden <a href="https://github.com/bitwarden/web/issues/426">ignores binary attachments</a> in its import functionality. Since I had less than ten of these, I simply migrated them manually by looking for <code>&lt;Binary/&gt;</code> entries in the KeePass XML to determine what they were, and then extracting them through the UI. For people who have many attachments, a tool is mentioned in the GitHub issue linked above that should transfer over KeePass attachments as well as references correctly.</p><p>So far, I&apos;ve only had my Bitwarden setup running for a few days, but it seems to be quite stable, and, with the exception of the VPN issue, I haven&apos;t encountered any real problems with it. I&apos;m pretty happy with the Windows and Android apps, as well as with the browser extensions. The auto-fill functionality feels like it&apos;s finally bringing me closer to the state of the art for the convergence of security and convenience, which is very exciting!</p>]]></content:encoded></item><item><title><![CDATA[Meteorological meanderings, part 3: geocoding and reverse geocoding]]></title><description><![CDATA[<p>Strictly speaking, this isn&apos;t meteorology. However, in order to display meteorological data for user-requested locations, we must be able to translate our users&apos; requests into coordinates.</p><p>If a user searches for the name of a city, we need to be able to get a set of coordinates</p>]]></description><link>https://trycatch.dev/blog/meteorological-meanderings-part-3-geocoding-and-reverse-geocoding/</link><guid isPermaLink="false">6003a7028ffe1e000179319b</guid><category><![CDATA[Weather]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Tue, 23 Feb 2021 00:58:32 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1599930113854-d6d7fd521f10?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDR8fGdsb2JlfGVufDB8fHw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1599930113854-d6d7fd521f10?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDR8fGdsb2JlfGVufDB8fHw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Meteorological meanderings, part 3: geocoding and reverse geocoding"><p>Strictly speaking, this isn&apos;t meteorology. However, in order to display meteorological data for user-requested locations, we must be able to translate our users&apos; requests into coordinates.</p><p>If a user searches for the name of a city, we need to be able to get a set of coordinates for that city, since our weather information is all based on coordinates. The process for doing such a search is called geocoding. Conversely, if we happen to have an interactive map and we let the user click on any point on it, it&apos;d be nice to display a friendly name for the location they clicked. Translating coordinates to the name (or names) of a location is called reverse geocoding.</p><p>The easy solution for handling geocoding is to use a third-party service, like <a href="https://developer.here.com/products/geocoding-and-search">HERE</a>. For low-volume traffic, using OpenStreetMap&apos;s own hosted Nominatim geocoder instance <a href="https://operations.osmfoundation.org/policies/nominatim/">could be an option</a> as well. Writing our own system from scratch is impractical, for a few reasons. First, there are available open source applications already out there. Second, it&apos;s quite a challenge to do fuzzy text matches against potentially gigantic datasets. And third, there would be a lot of work involved in importing data correctly in various formats from multiple data sources.</p><p>If we want to simply host our own geocoding system, however, we have a few options. There&apos;s <a href="https://github.com/osm-search/Nominatim">Nominatim</a> itself, which, as mentioned above, is used by OpenStreetMap. Alternatively, applications like <a href="https://github.com/pelias/pelias">Pelias</a> and <a href="https://github.com/gisgraphy/gisgraphy">Gisgraphy</a> can import data from a variety of sources. We&apos;ll choose Nominatim, if for no other reason, then to have the ability to compare our locally-installed version&apos;s results to those of the <a href="https://nominatim.openstreetmap.org">OSM-hosted</a> one. It&apos;s a great way to do a sanity check.</p><p>Before installing and configuring Nominatim, however, we must figure out how to get source data for it. Since Nominatim uses OpenStreetMap data readily, we can look at downloading it from <a href="https://download.geofabrik.de/">Geofabrik</a>. This Germany-based company generously provides free downloads of various OSM data extracts. To download US data, there is a North America sub-region file available, which is currently sitting at 7.1GB for the <code>.osm.pbf</code> (<a href="https://en.wikipedia.org/wiki/Protocol_Buffers">Protobuf</a>) version. But we don&apos;t want to import the entire thing. Assuming we&apos;re not trying to provide hyper-local weather information, there&apos;s no need to have exact building or even street data, which can take up a lot of space. What we want to do is filter out the data we don&apos;t need, and leave in just the data we do need. To filter the source data, we can use <a href="https://wiki.openstreetmap.org/wiki/Osmfilter">osmfilter</a> from <a href="https://gitlab.com/osm-c-tools/osmctools">osm-c-tools</a>. The <code>osmctools</code> package is available in Debian/Ubuntu and other Linux distros; pre-built Windows binaries exist as well. The filtering application takes in a wide range of parameters that let you specify exactly how you want to perform the filtering. It even has provisions for modifying data, not just removing it. An example of how we can filter out unwanted data might look like this:</p><pre><code class="language-sh">$ osmconvert source-data.osm.pbf --out-o5m &gt; source-data.o5m
$ osmfilter source-data.o5m --keep=&quot;name= and ( admin_level= or place= or boundary=administrative )&quot; --drop-version --out-o5m -o=filtered-data.o5m
$ osmconvert filtered-data.o5m --out-pbf &gt; filtered-data.osm.pbf</code></pre><p>Because osmfilter doesn&apos;t work with Protobuf files directly, we first have to convert our source data to the <code><a href="https://wiki.openstreetmap.org/wiki/O5m">o5m</a></code> format. <a href="https://wiki.openstreetmap.org/wiki/Osmconvert">Osmconvert</a>, another application from osm-c-tools, can easily do that for us, as seen above. Then we perform the filtering itself, and after that, we convert the filtered data back to the Protobuf format. With the current version of US data, the 7.1GB file becomes a mere 80MB after filtering and conversion.</p><p>While the filtering example above is relatively short, filters can get much more complicated. The <code>--keep</code> parameter, predictably, tells osmfilter which tags to keep. Tags are nothing more than key/value pairs, and they form the basis for how OSM data is categorized. For example, <code>boundary=administrative</code> will give us a lot of (but not all) governmental areas as large as countries and as small as neighborhoods. Since OSM data is crowdsourced and continually evolving, there are times when a tag is incorrectly used, or perhaps the wrong tag is used. Also, tagging standards for OSM data have not remained fixed, so obsolete tags can still be found in use in some locations. That&apos;s why the data filtering process isn&apos;t always straightforward. For city-level data, the filtering in the above example should get us pretty close to a good state. With some manual verification and filter tweaking, it should be pretty easy to get it to be &quot;good enough&quot;.</p><p>Once we have the filtered data, we can get to work on installing and configuring Nominatim. An easy way to do this is to use Docker. There&apos;s a user-friendly <a href="https://github.com/mediagis/nominatim-docker">GitHub repo simply called &quot;nominatim-docker&quot;</a> that contains all the necessary components to set this up. Following the repository&apos;s instructions for <a href="https://github.com/mediagis/nominatim-docker/tree/master/3.5">version 3.5</a> of Nominatim, our setup steps might look like this:</p><pre><code class="language-sh">$ mv filtered-data.osm.pbf /home/me/nominatimdata/
$ git clone https://github.com/mediagis/nominatim-docker.git
$ cd nominatim-docker/3.5
$ docker build --pull --rm -t nominatim .
$ docker run -t -v /home/me/nominatimdata:/data nominatim sh /app/init.sh /data/filtered-data.osm.pbf postgresdata 4
$ docker run --restart=always -p 7070:8080 -d --name nominatim -v /home/me/nominatimdata/postgresdata:/var/lib/postgresql/12/main nominatim bash /app/start.sh</code></pre><p>Building the Docker image locally allows for easy modifications to what&apos;s included in the image. It also, conveniently, doesn&apos;t force us to download a multi-gigabyte image, which these can be. The first call to <code>docker run</code> imports our filtered data into a Postgres database running inside the Docker container. The second call launches the actual web application.</p><p>Now that we have Nominatim running in a Docker container and serving HTTP requests on port 7070, we can <a href="https://nominatim.org/release-docs/latest/api/Overview/">use its API</a> to perform geocoding and reverse geocoding queries. We can also access port 7070 via a web browser and use the provided web interface to make test queries. <em>(Note: as of Nominatim 3.6, the web interface is in a <a href="https://github.com/osm-search/nominatim-ui">separate project</a>.)</em></p><p>An important addition to make to this setup involves search ranking. By default, roughly speaking, larger areas will rank higher in search results than smaller areas. That means cities will be ahead of similarly-named neighborhoods, and US states will be ahead of cities. This isn&apos;t always desirable. If we search for &quot;Austin&quot;, for example, the top result will be Austin County, Texas. That wouldn&apos;t necessarily be a major issue for weather data if the city of Austin resided in this county. But it doesn&apos;t. Austin is in Travis County, Texas. Chances are, when a user searches for &quot;Austin&quot;, they&apos;re looking for the city. We want some amount of intelligence in our search rankings to fix issues like this.</p><p>Enter: Wikipedia, the venerable source of information on virtually anything. We can use a subset of its data to improve our internal search rankings. By taking into account how many other Wikipedia articles link to the entries for our OSM locations, we can measure their relative popularity. In other words, with our Austin example above, there should be more Wikipedia articles linking to the city of Austin than articles linking to Austin County, and we can use that information to alter the search results to place the city higher in the rankings. In fact, Nominatim has <a href="https://nominatim.org/release-docs/latest/admin/Import/#wikipediawikidata-rankings">provisions</a> for doing just this. And if we perform the import process, we will see the first search result for &quot;Austin&quot; now return the city. With this change, our geocoding setup should be ready.</p><p>In the next post, we&apos;ll look at generating map tiles from OSM data for the purposes of displaying them in an interactive pan/zoom map.</p>]]></content:encoded></item><item><title><![CDATA[Meteorological meanderings, part 2: visualizations and long-range forecasts]]></title><description><![CDATA[<p>In the previous post, we used GDAL to query temperature information from a downloaded HRRR GRIB2 file. That query, however, was for a single point. Sometimes it helps to be able to see many points at once, preferably on a map. This can be useful for determining whether a particular</p>]]></description><link>https://trycatch.dev/blog/meteorological-meanderings-part-2-visualizations-and-long-range-forecasts/</link><guid isPermaLink="false">5ffbccc145abe5000188aad1</guid><category><![CDATA[Weather]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Mon, 01 Feb 2021 13:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1579003593419-98f949b9398f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDEyfHx3ZWF0aGVyfGVufDB8fHw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1579003593419-98f949b9398f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDEyfHx3ZWF0aGVyfGVufDB8fHw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Meteorological meanderings, part 2: visualizations and long-range forecasts"><p>In the previous post, we used GDAL to query temperature information from a downloaded HRRR GRIB2 file. That query, however, was for a single point. Sometimes it helps to be able to see many points at once, preferably on a map. This can be useful for determining whether a particular band &quot;looks right&quot;, as well as for doing sanity checks when writing code to query or visualize the data yourself.</p><h3 id="a-brief-look-at-qgis">A brief look at QGIS</h3><p>The best tool I&apos;ve found so far for visualizing meteorological data is <a href="https://www.qgis.org">QGIS</a>. It&apos;s an extremely powerful cross-platform piece of software, with many features, but we don&apos;t need the vast majority of them. To visualize a band from HRRR with QGIS, we can start by bringing in a world map to orient ourselves. Thankfully, that&apos;s just a few clicks away, thanks to built-in support for OpenStreetMap:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/01/qgis-osm.gif" class="kg-image" alt="Meteorological meanderings, part 2: visualizations and long-range forecasts" loading="lazy" width="1124" height="750"></figure><p>Once we have the map pulled up, we can load HRRR as a separate layer on top of the OSM layer:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/01/qgis-hrrr-1.gif" class="kg-image" alt="Meteorological meanderings, part 2: visualizations and long-range forecasts" loading="lazy" width="1124" height="750"></figure><p>The HRRR file appears twice in the file browser. That is because it can be loaded as either a normal raster file, or as a <a href="https://docs.qgis.org/3.16/en/docs/user_manual/working_with_mesh/mesh_properties.html">mesh</a> file. Loading it as a raster file is much faster, but it&apos;s more difficult to control the appearance of layers, especially temporal data, which is why the second file &#x2014; the mesh &#x2014; is chosen above. After the data is loaded, which can take a while (that part isn&apos;t shown), we can go through its properties to select the band we want to see:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/01/qgis-hrrr-temperature.gif" class="kg-image" alt="Meteorological meanderings, part 2: visualizations and long-range forecasts" loading="lazy" width="1124" height="750"></figure><p>Setting the opacity to 50%, as seen above, helps to retain that orientation given to us by the OSM layer. After we have the desired band showing, we can use the &quot;Identify Features&quot; mode in QGIS to get details about the band at specific locations:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/01/qgis-hrrr-temperature-data.gif" class="kg-image" alt="Meteorological meanderings, part 2: visualizations and long-range forecasts" loading="lazy" width="1124" height="750"></figure><p>In this particular data file, it&apos;s approximately -4 degrees Celsius in the vicinity of Indianapolis, Indiana. This is a very cursory look at QGIS, but it&apos;s enough to show how useful it can be for visualizing meteorological data.</p><h3 id="long-range-forecasts-and-their-projections">Long-range forecasts and their projections</h3><p>To get a long-range forecast, <a href="https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-forcast-system-gfs">GFS</a> is a useful model. It&apos;s global, and it goes out to more days in the future than is reasonable. GFS data, like that of HRRR, is also downloadable from NOMADS. Accessing it via GDAL, however, poses a slight problem. Armed with knowledge that temperature at 2m above ground level is in band 435, we should be able to do this query just like we did with HRRR:</p><pre><code>$ gdallocationinfo -b 435 -wgs84 gfs.t00z.pgrb2.0p25.f192 -86.1583502 39.7683331

Report:
  Location: (-345P,201L)

Location is off this file! No further details to report.</code></pre><p>Oops! That&apos;s not good. We&apos;re using the same coordinates, specified as WGS84. GFS is supposed to have data for the entire globe, so how can the specified location be off the grid? Before digging into that, let&apos;s extract the temperature band into a separate file, so that there&apos;s less data to deal with. To do that, we can use another GDAL tool, <code>gdal_translate</code>. Its primary purpose is translating between different raster data formats, but it can also be used to extract bands. This is how we can use it to extract a single band and, at the same time, convert it to GeoTIFF:</p><pre><code>$ gdal_translate -b 435 gfs.t00z.pgrb2.0p25.f192 gfs.t00z.pgrb2.0p25.f192-temperature.tif

Input file size is 1440, 721
0...10...20...30...40...50...60...70...80...90...100 - done.</code></pre><p>Now let&apos;s see what <code>gdalinfo</code> can tell us about this band from its own GeoTIFF file:</p><pre><code>Driver: GTiff/GeoTIFF
Files: gfs.t00z.pgrb2.0p25.f192-temperature.tif
Size is 1440, 721
Coordinate System is:
GEOGCRS[&quot;Coordinate System imported from GRIB file&quot;,
    DATUM[&quot;unnamed&quot;,
        ELLIPSOID[&quot;Sphere&quot;,6371229,0,
            LENGTHUNIT[&quot;metre&quot;,1,
                ID[&quot;EPSG&quot;,9001]]]],
    PRIMEM[&quot;Greenwich&quot;,0,
        ANGLEUNIT[&quot;degree&quot;,0.0174532925199433,
            ID[&quot;EPSG&quot;,9122]]],
    CS[ellipsoidal,2],
        AXIS[&quot;latitude&quot;,north,
            ORDER[1],
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433,
                ID[&quot;EPSG&quot;,9122]]],
        AXIS[&quot;longitude&quot;,east,
            ORDER[2],
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433,
                ID[&quot;EPSG&quot;,9122]]]]
Data axis to CRS axis mapping: 2,1
Origin = (-0.125000000000000,90.125000000000000)
Pixel Size = (0.250000000000000,-0.250000000000000)
Metadata:
  AREA_OR_POINT=Area
Image Structure Metadata:
  INTERLEAVE=BAND
Corner Coordinates:
Upper Left  (  -0.1250000,  90.1250000) (  0d 7&apos;30.00&quot;W, 90d 7&apos;30.00&quot;N)
Lower Left  (  -0.1250000, -90.1250000) (  0d 7&apos;30.00&quot;W, 90d 7&apos;30.00&quot;S)
Upper Right (     359.875,      90.125) (359d52&apos;30.00&quot;E, 90d 7&apos;30.00&quot;N)
Lower Right (     359.875,     -90.125) (359d52&apos;30.00&quot;E, 90d 7&apos;30.00&quot;S)
Center      ( 179.8750000,   0.0000000) (179d52&apos;30.00&quot;E,  0d 0&apos; 0.01&quot;N)
Band 1 Block=1440x1 Type=Float64, ColorInterp=Gray
  Description = 2[m] HTGL=&quot;Specified height level above ground&quot;
  Metadata:
    GRIB_COMMENT=Temperature [C]
    GRIB_DISCIPLINE=0(Meteorological)
    GRIB_ELEMENT=TMP
    GRIB_FORECAST_SECONDS=691200 sec
    GRIB_IDS=CENTER=7(US-NCEP) SUBCENTER=0 MASTER_TABLE=2 LOCAL_TABLE=1 SIGNF_REF_TIME=1(Start_of_Forecast) REF_TIME=2021-01-15T00:00:00Z PROD_STATUS=0(Operational) TYPE=1(Forecast)
    GRIB_PDS_PDTN=0
    GRIB_PDS_TEMPLATE_ASSEMBLED_VALUES=0 0 2 0 96 0 0 1 192 103 0 2 255 0 0
    GRIB_PDS_TEMPLATE_NUMBERS=0 0 2 0 96 0 0 0 1 0 0 0 192 103 0 0 0 0 2 255 0 0 0 0 0
    GRIB_REF_TIME=1610668800 sec UTC
    GRIB_SHORT_NAME=2-HTGL
    GRIB_UNIT=[C]
    GRIB_VALID_TIME=1611360000 sec UTC</code></pre><p>Yep, it&apos;s certainly the band we wanted. But above the band-specific information, there is a clue as to what&apos;s wrong. The reported corner coordinates are, approximately, (0, 90) at the upper left and (360, -90) at the lower right. While the Y-coordinates seem fine for latitude, the X-coordinates are not. We want to see longitude represented as -180 to 180, not 0 to 360. That explains why the location information request failed: -86.1583502 really is off the grid here!</p><p>To fix this issue, we can use GDAL&apos;s warping capability:</p><pre><code>$ gdalwarp -t_srs &quot;EPSG:4326&quot; -wo SOURCE_EXTRA=100 --config CENTER_LONG 0 gfs.t00z.pgrb2.0p25.f192-temperature.tif gfs.t00z.pgrb2.0p25.f192-temperature-warped.tif

Creating output file that is 1440P x 721L.
Processing gfs.t00z.pgrb2.0p25.f192-temperature.tif [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.</code></pre><p>The above command does a few things. First, it specifies a different coordinate reference system, or spatial reference system (SRS). <a href="https://epsg.io/4326">EPSG:4326</a> is another name for WGS84. Next, it grabs a bit of extra data for the warp procedure, to ensure no gaps exist when stitching. Finally, the config option <code>CENTER_LONG 0</code> is what tells GDAL to recenter the image, which will get us the coordinate range we want:</p><pre><code>$ gdalinfo gfs.t00z.pgrb2.0p25.f192-temperature-warped.tif

Driver: GTiff/GeoTIFF
Files: gfs.t00z.pgrb2.0p25.f192-temperature-warped.tif
Size is 1440, 721
Coordinate System is:
GEOGCRS[&quot;WGS 84&quot;,
    DATUM[&quot;World Geodetic System 1984&quot;,
        ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563,
            LENGTHUNIT[&quot;metre&quot;,1]]],
    PRIMEM[&quot;Greenwich&quot;,0,
        ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]],
    CS[ellipsoidal,2],
        AXIS[&quot;geodetic latitude (Lat)&quot;,north,
            ORDER[1],
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]],
        AXIS[&quot;geodetic longitude (Lon)&quot;,east,
            ORDER[2],
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]],
    ID[&quot;EPSG&quot;,4326]]
Data axis to CRS axis mapping: 2,1
Origin = (-179.999755859375000,90.125000000000000)
Pixel Size = (0.249999673885899,-0.249999847496904)
Metadata:
  AREA_OR_POINT=Area
Image Structure Metadata:
  INTERLEAVE=BAND
Corner Coordinates:
Upper Left  (-179.9997559,  90.1250000) (179d59&apos;59.12&quot;W, 90d 7&apos;30.00&quot;N)
Lower Left  (-179.9997559, -90.1248900) (179d59&apos;59.12&quot;W, 90d 7&apos;29.60&quot;S)
Upper Right ( 179.9997745,  90.1250000) (179d59&apos;59.19&quot;E, 90d 7&apos;30.00&quot;N)
Lower Right ( 179.9997745, -90.1248900) (179d59&apos;59.19&quot;E, 90d 7&apos;29.60&quot;S)
Center      (   0.0000093,   0.0000550) (  0d 0&apos; 0.03&quot;E,  0d 0&apos; 0.20&quot;N)
Band 1 Block=1440x1 Type=Float64, ColorInterp=Gray
  Description = 2[m] HTGL=&quot;Specified height level above ground&quot;
  Metadata:
    GRIB_COMMENT=Temperature [C]
    GRIB_DISCIPLINE=0(Meteorological)
    GRIB_ELEMENT=TMP
    GRIB_FORECAST_SECONDS=691200 sec
    GRIB_IDS=CENTER=7(US-NCEP) SUBCENTER=0 MASTER_TABLE=2 LOCAL_TABLE=1 SIGNF_REF_TIME=1(Start_of_Forecast) REF_TIME=2021-01-15T00:00:00Z PROD_STATUS=0(Operational) TYPE=1(Forecast)
    GRIB_PDS_PDTN=0
    GRIB_PDS_TEMPLATE_ASSEMBLED_VALUES=0 0 2 0 96 0 0 1 192 103 0 2 255 0 0
    GRIB_PDS_TEMPLATE_NUMBERS=0 0 2 0 96 0 0 0 1 0 0 0 192 103 0 0 0 0 2 255 0 0 0 0 0
    GRIB_REF_TIME=1610668800 sec UTC
    GRIB_SHORT_NAME=2-HTGL
    GRIB_UNIT=[C]
    GRIB_VALID_TIME=1611360000 sec UTC</code></pre><p>That looks much better. The upper left coordinate is now (-180, 90) and lower right is (180, -90). This is what we expect to see. Now that we&apos;ve confirmed that the corner coordinates look good, we can perform our temperature query against the warped file:</p><pre><code>$ gdallocationinfo -wgs84 gfs.t00z.pgrb2.0p25.f192-temperature-warped.tif -86.1583502 39.7683331

Report:
  Location: (375P,201L)
  Band 1:
    Value: -1.61920776367185</code></pre><p>Hooray, it works! Using this procedure, we can convert any band to an easily queryable format. And, just like with HRRR, if we bring in more bands, we should be able to generate a multi-day forecast from GFS data.</p><p>We&apos;re not going to cover this now, but there&apos;s another interesting conversion process we could get involved in, if we wanted to access global forecasting data from Deutscher Wetterdienst, the German Meteorological Service. They produce the ICON model, which uses a triangle-based grid system. Thankfully, they have an <a href="https://www.dwd.de/DE/leistungen/opendata/help/modelle/Opendata_cdo_EN.pdf?__blob=publicationFile&amp;v=3">extraordinarily well-written document</a> describing how to convert that data to the familiar GRIB2 format.</p><p>Next up, we&apos;ll switch gears and look at performing geocoding and reverse geocoding using a locally-hosted service.</p>]]></content:encoded></item><item><title><![CDATA[Meteorological meanderings, part 1: getting short-range US forecasts]]></title><description><![CDATA[<p>NOAA&apos;s NCEP generates a lot of weather data. Various numerical weather prediction models are executed at regular intervals multiple times a day, and their outputs are made available as FTP and HTTPS downloads. Many of these models have significant overlap in the data they generate, so a natural</p>]]></description><link>https://trycatch.dev/blog/meteorological-meanderings-part-1-getting-short-range-us-forecasts/</link><guid isPermaLink="false">5ff53750b6198a00019061b7</guid><category><![CDATA[Weather]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Sat, 23 Jan 2021 02:22:40 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1561484930-974554019ade?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDl8fHdlYXRoZXJ8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1561484930-974554019ade?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDl8fHdlYXRoZXJ8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Meteorological meanderings, part 1: getting short-range US forecasts"><p>NOAA&apos;s NCEP generates a lot of weather data. Various numerical weather prediction models are executed at regular intervals multiple times a day, and their outputs are made available as FTP and HTTPS downloads. Many of these models have significant overlap in the data they generate, so a natural question would be, why not consolidate? The answer is that, because our planet&apos;s atmosphere is chaotic, no numerical model gets everything right. Some models have been shown to make better predictions in one situation, while others are superior in another situation. Therefore, running multiple models gives meteorologists a chance to compare and contrast results in order to make better overall predictions.</p><p>For short-range CONUS (CONtiguous United States, no Alaska or Hawaii) forecasts, two models often referenced are <a href="https://www.emc.ncep.noaa.gov/emc/pages/numerical_forecast_systems/nam.php">NAM</a> (North American Mesoscale) and <a href="https://rapidrefresh.noaa.gov/">RAP/HRRR</a> (RAPid Refresh and High Resolution Rapid Refresh, respectively). NAM has been around since the early 2000s, while RAP was introduced in the 2010s. They run at different intervals and have different resolutions. HRRR has an improved resolution over RAP. The latest version of RAP and HRRR became operational in December 2020. Let&apos;s take a look at how we can use HRRR for short-range forecasts.</p><p>Although some APIs are available from NOAA/NWS, it looks like they aren&apos;t used very much. Instead, files with predictably-formatted names are uploaded to older FTP sites and to <a href="https://nomads.ncep.noaa.gov/">NOMADS</a> (NOAA Operational Model Archive and Distribution System) during model runs. Then, people use scripts to download and process those files, a concept referred to as <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load">ETL</a>.</p><p>For HRRR, the data files themselves are in the <a href="https://en.wikipedia.org/wiki/GRIB">GRIB Edition 2</a> format, as standardized by the <a href="https://public.wmo.int/en">WMO</a> (World Meteorological Organization, a UN agency). GRIB2 files can contain multiple data sets, or bands, inside them. For example, one band might be temperature at 2 meters above ground, while another band might be snow depth on the ground. Definitions for the bands can be found on <a href="https://www.nco.ncep.noaa.gov/pmb/docs/grib2/">NCEP&apos;s GRIB2 pages</a>. These bands contain data encoded in a grid pattern, based on a defined map projection for the grid. The current version of HRRR uses a <a href="https://en.wikipedia.org/wiki/Lambert_conformal_conic_projection">Lambert conformal conic projection</a> at a 3 km resolution.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2021/01/image.png" class="kg-image" alt="Meteorological meanderings, part 1: getting short-range US forecasts" loading="lazy" width="835" height="841" srcset="https://trycatch.dev/content/images/size/w600/2021/01/image.png 600w, https://trycatch.dev/content/images/2021/01/image.png 835w" sizes="(min-width: 720px) 720px"><figcaption>Screenshot of https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20210106/conus/</figcaption></figure><p>The above screenshot is just the top of the page. There are hundreds of files with similar-looking names in that directory. With basic scripting, it&apos;s fairly easy to download the ones you need. Determining what you need, however, can be tricky. Looking at the <a href="https://www.nco.ncep.noaa.gov/pmb/products/hrrr/">NCEP Products Inventory page for HRRR</a> is a first step. This page explains, among other things, the formatting of the filenames. The double zero in <code>t00z</code> represents the time the model ran, and the increasing double-digit number after the &quot;f&quot; is the forecast hour. This page also tells us that the &quot;wrfnat&quot; part of the filenames seen above represents Native Levels. Some searching leads to an <a href="https://rapidrefresh.noaa.gov/faq/HRRR.faq.html">FAQ page</a> (with Fortran code, in 2021 no less) mentioning that native levels are sigma levels, and a bit more searching tells us that this is referring to the <a href="https://en.wikipedia.org/wiki/Sigma_coordinate_system">sigma coordinate system</a>. Sure enough, opening one of the human-readable index (<code>.idx</code>) files reveals a whole lot of hybrid level data:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2021/01/image-1.png" class="kg-image" alt="Meteorological meanderings, part 1: getting short-range US forecasts" loading="lazy" width="491" height="890"></figure><p>Speaking of index files, these are used to help reduce bandwidth utilization when downloading model data. Utilizing <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests">HTTP Range requests</a>, with the help of these index files, it&apos;s possible to download only the needed GRIB2 bands, rather than entire sets of these large files.</p><p>In addition to native level data, there are sub-hourly outputs, 3D pressure levels, and 2D surface levels. It looks like &quot;wrfsfc&quot;, 2D surface levels, is going to be useful for displaying basic forecast data, so let&apos;s stick with that set of files.</p><p>How do we begin processing and analyzing these GRIB2 files, then? There are a few different applications and libraries that can work with this format. For both command-line and programmatic access, I&apos;ve found <a href="https://gdal.org/">GDAL</a> to be a most useful library. Not only does it have a powerful set of CLI tools, but it also comes with a C/C++ API, along with Python and Java bindings. Debian and Ubuntu have a package ready for installation, named <code><a href="https://packages.debian.org/sid/gdal-bin">gdal-bin</a></code>; many other Linux distros have packages as well, and there are multiple sources for pre-built Windows binaries, such as <a href="https://www.gisinternals.com/release.php">GISInternals</a>. Executing the <code>gdalinfo</code> binary against a downloaded HRRR file will yield pages of information, starting with this:</p><!--kg-card-begin: html--><pre style="overflow: auto; max-height: 600px;"><code>
Driver: GRIB/GRIdded Binary (.grb, .grb2)
Files: hrrr.t00z.wrfsfcf12.grib2
Size is 1799, 1059
Coordinate System is:
PROJCRS[&quot;unnamed&quot;,
    BASEGEOGCRS[&quot;Coordinate System imported from GRIB file&quot;,
        DATUM[&quot;unnamed&quot;,
            ELLIPSOID[&quot;Sphere&quot;,6371229,0,
                LENGTHUNIT[&quot;metre&quot;,1,
                    ID[&quot;EPSG&quot;,9001]]]],
        PRIMEM[&quot;Greenwich&quot;,0,
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433,
                ID[&quot;EPSG&quot;,9122]]]],
    CONVERSION[&quot;Lambert Conic Conformal (2SP)&quot;,
        METHOD[&quot;Lambert Conic Conformal (2SP)&quot;,
            ID[&quot;EPSG&quot;,9802]],
        PARAMETER[&quot;Latitude of false origin&quot;,38.5,
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],
            ID[&quot;EPSG&quot;,8821]],
        PARAMETER[&quot;Longitude of false origin&quot;,262.5,
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],
            ID[&quot;EPSG&quot;,8822]],
        PARAMETER[&quot;Latitude of 1st standard parallel&quot;,38.5,
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],
            ID[&quot;EPSG&quot;,8823]],
        PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,38.5,
            ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],
            ID[&quot;EPSG&quot;,8824]],
        PARAMETER[&quot;Easting at false origin&quot;,0,
            LENGTHUNIT[&quot;metre&quot;,1],
            ID[&quot;EPSG&quot;,8826]],
        PARAMETER[&quot;Northing at false origin&quot;,0,
            LENGTHUNIT[&quot;metre&quot;,1],
            ID[&quot;EPSG&quot;,8827]]],
    CS[Cartesian,2],
        AXIS[&quot;(E)&quot;,east,
            ORDER[1],
            LENGTHUNIT[&quot;Metre&quot;,1]],
        AXIS[&quot;(N)&quot;,north,
            ORDER[2],
            LENGTHUNIT[&quot;Metre&quot;,1]]]
Data axis to CRS axis mapping: 1,2
Origin = (-2699020.142521930858493,1588193.847443336388096)
Pixel Size = (3000.000000000000000,-3000.000000000000000)
Corner Coordinates:
Upper Left  (-2699020.143, 1588193.847) (134d 7&apos;17.14&quot;W, 47d50&apos;44.64&quot;N)
Lower Left  (-2699020.143,-1588806.153) (122d43&apos;44.80&quot;W, 21d 7&apos;19.89&quot;N)
Upper Right ( 2697979.857, 1588193.847) ( 60d53&apos;28.48&quot;W, 47d50&apos;57.51&quot;N)
Lower Right ( 2697979.857,-1588806.153) ( 72d16&apos;48.48&quot;W, 21d 7&apos;28.62&quot;N)
Center      (    -520.143,    -306.153) ( 97d30&apos;21.52&quot;W, 38d29&apos;50.09&quot;N)
Band 1 Block=1799x1 Type=Float64, ColorInterp=Undefined
  Description = 0[-] EATM=&quot;Entire Atmosphere&quot;
  Metadata:
    GRIB_COMMENT=Maximum / Composite radar reflectivity [dB]
    GRIB_DISCIPLINE=0(Meteorological)
    GRIB_ELEMENT=REFC
    GRIB_FORECAST_SECONDS=43200 sec
    GRIB_IDS=CENTER=7(US-NCEP) SUBCENTER=0 MASTER_TABLE=2 LOCAL_TABLE=1 SIGNF_REF_TIME=1(Start_of_Forecast) REF_TIME=2021-01-10T00:00:00Z PROD_STATUS=0(Operational) TYPE=1(Forecast)
    GRIB_PDS_PDTN=0
    GRIB_PDS_TEMPLATE_ASSEMBLED_VALUES=16 196 2 0 83 0 0 1 12 10 0 0 255 0 0
    GRIB_PDS_TEMPLATE_NUMBERS=16 196 2 0 83 0 0 0 1 0 0 0 12 10 0 0 0 0 0 255 0 0 0 0 0
    GRIB_REF_TIME=  1610236800 sec UTC
    GRIB_SHORT_NAME=0-EATM
    GRIB_UNIT=[dB]
    GRIB_VALID_TIME=  1610280000 sec UTC
Band 2 Block=1799x1 Type=Float64, ColorInterp=Undefined
  Description = 0[-] CTL=&quot;Level of cloud tops&quot;
  Metadata:
    GRIB_COMMENT=Echo Top [m]
    GRIB_DISCIPLINE=0(Meteorological)
    GRIB_ELEMENT=RETOP
    GRIB_FORECAST_SECONDS=43200 sec
    GRIB_IDS=CENTER=7(US-NCEP) SUBCENTER=0 MASTER_TABLE=2 LOCAL_TABLE=1 SIGNF_REF_TIME=1(Start_of_Forecast) REF_TIME=2021-01-10T00:00:00Z PROD_STATUS=0(Operational) TYPE=1(Forecast)
    GRIB_PDS_PDTN=0
    GRIB_PDS_TEMPLATE_ASSEMBLED_VALUES=16 3 2 0 83 0 0 1 12 3 0 0 255 0 0
    GRIB_PDS_TEMPLATE_NUMBERS=16 3 2 0 83 0 0 0 1 0 0 0 12 3 0 0 0 0 0 255 0 0 0 0 0
    GRIB_REF_TIME=  1610236800 sec UTC
    GRIB_SHORT_NAME=0-CTL
    GRIB_UNIT=[m]
    GRIB_VALID_TIME=  1610280000 sec UTC
Band 3 Block=1799x1 Type=Float64, ColorInterp=Undefined
  Description = 0[-] EATM=&quot;Entire Atmosphere&quot;
  Metadata:
    GRIB_COMMENT=(prodType 0, cat 16, subcat 201) [-]
    GRIB_DISCIPLINE=0(Meteorological)
    GRIB_ELEMENT=unknown
    GRIB_FORECAST_SECONDS=43200 sec
    GRIB_IDS=CENTER=7(US-NCEP) SUBCENTER=0 MASTER_TABLE=2 LOCAL_TABLE=1 SIGNF_REF_TIME=1(Start_of_Forecast) REF_TIME=2021-01-10T00:00:00Z PROD_STATUS=0(Operational) TYPE=1(Forecast)
    GRIB_PDS_PDTN=0
    GRIB_PDS_TEMPLATE_ASSEMBLED_VALUES=16 201 2 0 83 0 0 1 12 10 0 0 255 0 0
    GRIB_PDS_TEMPLATE_NUMBERS=16 201 2 0 83 0 0 0 1 0 0 0 12 10 0 0 0 0 0 255 0 0 0 0 0
    GRIB_REF_TIME=  1610236800 sec UTC
    GRIB_SHORT_NAME=0-EATM
    GRIB_UNIT=[-]
    GRIB_VALID_TIME=  1610280000 sec UTC
</code></pre><!--kg-card-end: html--><p>Only the first three bands are shown above, but this particular file has 173 of them. As the filename <code>hrrr.t00z.wrfsfcf12.grib2</code> indicates, this file was generated in the midnight cycle (<code>t00z</code>), and the data it contains is 12 hours in the future (<code>f12</code>). &#xA0;The intimidating <code>PROJCRS[...]</code> multi-line string is the <a href="https://www.ogc.org/standards/wkt-crs">WKT ISO standard</a> way of describing coordinate reference systems. After that, <code>gdalinfo</code> outputs metadata about every single band in the file.</p><p>How do we get some actual information out of this thing? First, we need to find a band that we care about. Let&apos;s look at temperature, as an example:</p><figure class="kg-card kg-code-card"><pre><code>Band 71 Block=1799x1 Type=Float64, ColorInterp=Undefined
  Description = 2[m] HTGL=&quot;Specified height level above ground&quot;
  Metadata:
    GRIB_COMMENT=Temperature [C]
    GRIB_DISCIPLINE=0(Meteorological)
    GRIB_ELEMENT=TMP
    GRIB_FORECAST_SECONDS=43200 sec
    GRIB_IDS=CENTER=7(US-NCEP) SUBCENTER=0 MASTER_TABLE=2 LOCAL_TABLE=1 SIGNF_REF_TIME=1(Start_of_Forecast) REF_TIME=2021-01-10T00:00:00Z PROD_STATUS=0(Operational) TYPE=1(Forecast)
    GRIB_PDS_PDTN=0
    GRIB_PDS_TEMPLATE_ASSEMBLED_VALUES=0 0 2 0 83 0 0 1 12 103 0 2 255 0 0
    GRIB_PDS_TEMPLATE_NUMBERS=0 0 2 0 83 0 0 0 1 0 0 0 12 103 0 0 0 0 2 255 0 0 0 0 0
    GRIB_REF_TIME=  1610236800 sec UTC
    GRIB_SHORT_NAME=2-HTGL
    GRIB_UNIT=[C]
    GRIB_VALID_TIME=  1610280000 sec UTC</code></pre><figcaption>Band 71 output from gdalinfo</figcaption></figure><p>In this file, band 71 contains the temperature in degrees Celsius at 2m above ground level. (Strictly speaking, the GRIB2 temperature data is in Kelvin, but <a href="https://gdal.org/drivers/raster/grib.html">GDAL automatically converts it</a>.) Some of the metadata fields to pay special attention to are <code>GRIB_ELEMENT</code> and <code>GRIB_SHORT_NAME</code>, because in many situations, a combination of these two will uniquely identify a data band that is of interest to us. Since we know which band we want to look at, we can query it using another GDAL binary:</p><pre><code>$ gdallocationinfo -b 71 -wgs84 hrrr.t00z.wrfsfcf12.grib2 -86.1583502 39.7683331

Report:
  Location: (1222P,462L)
  Band 71:
    Value: -3.91000976562498</code></pre><p><code>gdallocationinfo</code> accepts a band filter, which we use to only have it show band 71. <code>-wgs84</code> tells the application that the (x,y) coordinates are specified in the <a href="https://en.wikipedia.org/wiki/World_Geodetic_System">WGS84 format</a>, which is the familiar latitude and longitude system that we see almost everywhere. Of course, since it expects the order of parameters to be &quot;filename x y&quot;, we&apos;re really specifying longitude first and latitude second. In the above example, the coordinates are in Indianapolis, Indiana, USA. The predicted temperature is -3.9&#xB0;C.</p><p>Now that we&apos;ve figured out how to get temperature, let&apos;s take a look at some other bands that might be useful for a forecast:</p><!--kg-card-begin: markdown--><ul>
<li>Pressure
<ul>
<li>Element <code>PRES</code> at <code>0-SFC</code> (surface level)</li>
</ul>
</li>
<li>Relative humidity percentage
<ul>
<li>Element <code>RH</code> at <code>2-HTGL</code> (2m height above ground level)</li>
</ul>
</li>
<li>Precipitation rate
<ul>
<li>Element <code>PRATE</code> at <code>0-SFC</code></li>
</ul>
</li>
<li>Precipitation categories, represented as Boolean values
<ul>
<li>Rain: <code>CRAIN</code></li>
<li>Snow: <code>CSNOW</code></li>
<li>Freezing rain: <code>CFRZR</code></li>
<li>Ice pellets (sleet): <code>CICEP</code></li>
</ul>
</li>
<li>Total cloud cover percentage
<ul>
<li>Element <code>TCDC</code> at <code>0-EATM</code> (entire atmosphere)</li>
</ul>
</li>
<li>Wind</li>
</ul>
<!--kg-card-end: markdown--><p>Wind is a special case here. For wind, we have two bands: the u-component (<code>UGRD</code>) and the v-component (<code>VGRD</code>). As explained by <a href="http://tornado.sfsu.edu/geosciences/classes/m430/Wind/WindDirection.html">this university document</a>, a positive value in the u-component denotes wind blowing eastward, while a positive v-component denotes wind blowing northward. Conversely, negative amounts denote westward and southward winds, respectively. The document also explains how to convert from these vector components to more familiar azimuth angle and total speed.</p><p>The same kind of <code>gdallocationinfo</code> query as we saw above can be used to get data for all of these bands. Adding perhaps a few more bands and downloading data for more forecast times, we can get a short-range forecast going!</p><p>In the next post, we&apos;ll look at visualizing GRIB2 bands as well as dealing with long-range, global forecasts.</p>]]></content:encoded></item><item><title><![CDATA[Meteorological meanderings, part 0: a software developer meets exotic disciplines]]></title><description><![CDATA[<p>I&apos;ve been writing software in one form or another since I was a kid. Starting out with BASIC and moving through various languages and frameworks has always been an interesting process of discovery due to the ideas in the new materials being in opposition to my preconceived notions,</p>]]></description><link>https://trycatch.dev/blog/meteorological-meanderings-part-0-a-software-developer-meets-exotic-disciplines/</link><guid isPermaLink="false">5fe2aee15e440d0001af5421</guid><category><![CDATA[Weather]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Sat, 23 Jan 2021 01:54:05 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1469365556835-3da3db4c253b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fGF0bW9zcGhlcmV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1469365556835-3da3db4c253b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fGF0bW9zcGhlcmV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Meteorological meanderings, part 0: a software developer meets exotic disciplines"><p>I&apos;ve been writing software in one form or another since I was a kid. Starting out with BASIC and moving through various languages and frameworks has always been an interesting process of discovery due to the ideas in the new materials being in opposition to my preconceived notions, or simply due to a lack of experience with a particular concept. Such challenges are generally what I would consider to be immensely fun, so I tend to revel in them. And besides, when I get stuck, there are plenty of blog posts and StackOverflow answers to help me figure out most problems.</p><p>A few months back, I decided to take on a new side project: a weather forecasting website/app. Why? Well, there are a few reasons. First, because I don&apos;t like any of the current popular weather destinations: they are full of ads, or are painful to use (sometimes with a UI reminiscent of the early 2000s Web), or have been <a href="https://blog.darksky.net/dark-sky-has-a-new-home/">greedily gobbled up and shuttered</a> with the ostensible purposes of &quot;help[ing] as many people as we can&quot; &#x2014; give me a break! Another reason is that I&apos;ve always been curious about how weather information is distributed and what such data looks like. And finally, because I&apos;ve been missing the world of .NET, while at the same time, I&apos;ve wanted to explore the recent resurgence of server-side web rendering with progressive client-side enhancement with technologies like <a href="https://htmx.org/">htmx</a>. I&apos;d also hoped to work with a couple people on this as a way to hang out and share knowledge (to me, this is much more fun than board games or video games), but thus far, scheduling has admittedly been a bit of a challenge.</p><h2 id="initial-thoughts">Initial thoughts</h2><p>I started by trying to imagine roughly what I would like in a weather website, and then figuring out what to research after that. The first thing I would want is a nice map that supports pan and zoom. But I didn&apos;t want to use existing services for that, as I wanted to minimize reliance on third parties except where absolutely necessary. This took me down a very long rabbit hole of exploring the world of Geographic Information Systems, or GIS. Map projections, geographical and geopolitical categorizations, and various open source tools to convert between a multitude of formats as well as to filter data down to what I&apos;d need for weather: these have all been swimming in my head.</p><p>In addition to mapping, and, arguably, the most important thing to figure out, is how and where to get the actual weather data! The obvious high-level answer is the National Oceanic and Atmospheric Administration (NOAA, pronounced like Noah). Within NOAA, however, are a multitude of agencies, divisions, and what-have-you. The agency of interest here is the National Weather Service (NWS), and the National Centers for Environmental Prediction (NCEP) is the part of NWS that is responsible for things like forecasts. As its name implies, NCEP has multiple centers with differing responsibilities. The Environmental Modeling Center (EMC) is of particular interest here. Rather, the data that is generated by this center and made freely available to the world is what is of interest. There are many gigabytes, maybe terabytes, of data being generated within NCEP every day. They have supercomputers crunching numbers non-stop. The resulting data has lots of different shapes and sizes, and sometimes it&apos;s pretty difficult to find concrete definitions and explanations for what I&apos;m looking at that are comprehensible to a non-meteorologist like myself.</p><h2 id="figuring-things-out-step-by-step">Figuring things out, step by step</h2><p>I&apos;m still in the middle of all this, and, while I have learned a lot, there is plenty more that I&apos;ll need to understand in order to make a useful weather site. I&apos;ll delve deeper into what I&apos;ve been able to gather, deduce, or just guess, in later posts.</p><p>It does often feel like I&apos;m being overly presumptuous in my ability to do this project. GIS and meteorology are fields that people study for years, and I&apos;m just stumbling through them. I know I&apos;ll be making plenty of mistakes and erroneous assumptions along the way, but hopefully I&apos;ll recognize and correct them later on.</p><p>The biggest challenge so far, as I&apos;ve already alluded to, has been my knowledge gap. I can search the GIS StackExchange all I want, but a lot of the questions and answers I simply don&apos;t understand. Additionally, I&apos;ve encountered situations where people have (apparently) solved some rudimentary issues that I&apos;d faced, but it would seem that nobody is talking about the problems or the solutions. It is, of course, possible that the terminology is just so foreign to me that I don&apos;t know how to ask the questions. While I generally consider myself to be a fairly competent software developer, trying to work through some of these problems has certainly been a humbling experience. For as much as (I think) I know about software, there are entire universes of information and fields of study that are just completely orthogonal to everything I&apos;ve learned so far, and starting out in them is eye-opening.</p><p>In upcoming posts, I&apos;ll discuss some of the problems and solutions I&apos;ve dealt with on this journey, many of which might seem relatively simple. Some of them are, while others have been deceptively difficult to figure out. Interestingly, just like with good code, the solutions to many problems are concise, while the process of discovering them has been anything but.</p><p>Until recently, most of this information has been, as Donald Rumsfeld would say, <em>unknown unknowns</em> for me. Now, it&apos;s slowly getting into the <em>known unknowns</em> territory. Join me in unraveling these mysteries!</p>]]></content:encoded></item><item><title><![CDATA[Self-hosting a streaming video platform]]></title><description><![CDATA[<p>Before the pandemic began, I used to host movie nights. Since that activity became dangerous to do in person, I wanted to come up with a virtual alternative. Having set up a <a href="https://trycatch.dev/blog/self-hosting-a-video-chat-system/">self-hosted video chat system</a>, this would be an additional project that&apos;d work alongside with the VPS-hosted,</p>]]></description><link>https://trycatch.dev/blog/self-hosting-a-streaming-video-platform/</link><guid isPermaLink="false">5f5672bfaf1dc10001ff07a9</guid><category><![CDATA[Docker]]></category><category><![CDATA[Linux]]></category><category><![CDATA[Self-hosting]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Sat, 12 Sep 2020 16:25:47 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1478720568477-152d9b164e26?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1478720568477-152d9b164e26?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Self-hosting a streaming video platform"><p>Before the pandemic began, I used to host movie nights. Since that activity became dangerous to do in person, I wanted to come up with a virtual alternative. Having set up a <a href="https://trycatch.dev/blog/self-hosting-a-video-chat-system/">self-hosted video chat system</a>, this would be an additional project that&apos;d work alongside with the VPS-hosted, Dockerized, LDAP-secured environment described in the post linked at the beginning of this unnecessarily long sentence.</p><h2 id="background">Background</h2><p>What am I trying to accomplish? I&apos;d like to have a way for everyone attending a virtual movie night to watch the same thing at the same time, with reasonable picture and audio quality, and without having to download and install a bunch of stuff. It&apos;d also be nice to be able to chat (by text) during the movie.</p><p>The text chat part is easily accomplished. Using the self-hosted Jitsi Meet instance, we usually hang out and talk in there before starting the movie. During the movie we simply turn off video and audio and just use the text chat provided by Jitsi Meet. It&apos;s not fancy by any means, but it&apos;s sufficient.</p><p>As for the movie streaming, that gets a bit more interesting. First, let&apos;s start with the obligatory disclaimer: <strong>don&apos;t break copyright laws in your country</strong>. For the purpose of providing a realistic example in this post, I&apos;m going to use the movie <em>Night of the Living Dead</em>, downloaded from the <a href="https://archive.org/details/NightOfTheLivingDead-MPEG">Internet Archive</a>, as it is in the public domain.</p><p>There are a number of popular streaming services that a lot of people use, such as YouTube Live, Twitch, and Facebook streaming. Just like in the previous post, however, I wanted to self-host instead because of privacy and the &quot;can I do it well enough?&quot; experimentation factor.</p><h2 id="the-tools">The tools</h2><p>It turns out, there are great open source tools available for doing this kind of streaming setup. On the server side, you need an application to receive streaming data and save it appropriately for serving to viewers. A web application for doing the actual serving of the data stream is needed as well. On the data source side, an application must send the stream to the server. (And on the viewer side, just a web browser is needed.)</p><h3 id="the-server-side">The server side</h3><p>A great choice for receiving the stream, saving it, and being able to serve it is everyone&apos;s favorite web server, <a href="https://nginx.org/">nginx</a>, along with <a href="https://github.com/arut/nginx-rtmp-module">nginx-rtmp-module</a> (available in Debian/Ubuntu as <a href="https://packages.debian.org/buster/libnginx-mod-rtmp">libnginx-mod-rtmp</a>). Both can be installed easily in a Docker container. RTMP is a technology that hails from the olden days of Flash video, developed by Macromedia. These days, however, it has become the big standard supported by the major streaming platforms for sending audio and video data with minimal latency.</p><p>To save and later serve the data sent through RTMP, the aforementioned nginx module offers support for <a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming">HLS</a>, one of the most popular stream serving technologies today. The module has the ability to save an entire stream for later viewing (useful for playing stored videos) as well as the option to only save a minimal amount of cached data during a live event for immediate transmission. I chose the second option because then I don&apos;t have to worry about available disk space on the host.</p><p>For serving the HLS stream from the server to viewers, an HTML/JavaScript player application is needed. There are a number of choices, such as <a href="https://github.com/videojs/video.js">Video.js</a>, <a href="https://github.com/mediaelement/mediaelement/">MediaElement.js</a>, and <a href="https://github.com/sampotts/plyr">Plyr</a>. After using Video.js for a while, I decided to switch to Plyr because of its smoother and more polished-looking interface:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2020/09/image-11.png" class="kg-image" alt="Self-hosting a streaming video platform" loading="lazy" width="1554" height="939" srcset="https://trycatch.dev/content/images/size/w600/2020/09/image-11.png 600w, https://trycatch.dev/content/images/size/w1000/2020/09/image-11.png 1000w, https://trycatch.dev/content/images/2020/09/image-11.png 1554w" sizes="(min-width: 720px) 720px"><figcaption>See the nice control bar on the bottom?</figcaption></figure><p>Incidentally, it helps to have a non-movie image or clip playing beforehand to allow everyone to tune in and get comfortable. All a viewer needs is a web browser to enjoy the show.</p><h3 id="the-data-source-side">The data source side</h3><p>Once an RTMP target is configured, the next logical question is, what&apos;s the source? The answer, at least with my setup, is <a href="https://obsproject.com/">OBS Studio</a>. This cross-platform, open source software is a popular choice for recording as well as live streaming. It can stream from multiple data sources arranged in a variety of ways on the screen. However, most of its power remains unused in a movie night setup.</p><h2 id="the-server-configuration">The server configuration</h2><p>Now that all the tools are ready, it&apos;s time to configure them. For nginx and the RTMP plugin, the configuration is a bit complex, but it can be broken down into parts.</p><figure class="kg-card kg-code-card"><pre><code class="language-nginx">http {
        ##
        # Basic Settings
        ##

        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;

        include /etc/nginx/mime.types;
        default_type application/octet-stream;

        ##
        # Logging Settings
        ##

        access_log /var/log/nginx/access.log;
        error_log /var/log/nginx/error.log;

        ##
        # Gzip Settings
        ##

        gzip on;

        ##
        # Virtual Host Configs
        ##

        server_tokens off;

        server {
                listen 8080;

                location / {
                        root /data/plyr;
                }

                location /livestream {
                        add_header Cache-Control &apos;no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0&apos;;
                        if_modified_since off;
                        expires off;
                        etag off;
                        types {
                                application/vnd.apple.mpegurl m3u8;
                                video/mp2t ts;
                        }
                        alias /data/hls/livestream;
                }
        }
}

rtmp {
        server {
                listen 1935;
                chunk_size 2048;

                on_publish http://127.0.0.1:3000/auth;

                application stream {
                        live on;
                        record off;
                        meta copy;

                        wait_key on;
                        wait_video on;

                        hls on;
                        hls_path /data/hls/livestream;
                        hls_fragment 4;
                        hls_playlist_length 16;
                        hls_sync 100ms;
                }
        }
}</code></pre><figcaption>Excerpt from nginx.conf</figcaption></figure><p>That&apos;s a lot of stuff! Let&apos;s go from the top down and analyze this configuration file. On the first line is the beginning of the <code>http</code> block. The <code>http</code> block handles the web server configuration for nginx. After some basic settings (most of them default) we have a <code>server</code> block. The <code>listen 8080;</code> directive instructs nginx to listen on port 8080 for HTTP traffic. The <code>location /</code> block beginning on line 37 configures the root website, e.g., <code>http://streaming.example.com/</code>, to get its data from <code>/data/plyr</code>. That location is where I have my Plyr HTML/JS code stored for loading the web player.</p><p>The next block, <code>location /livestream</code> beginning on line 41, is where the HLS data (the playlists and the video streams) will be served, e.g., <code>http://streaming.example.com/livestream</code>. Inside this block are some directives to prevent caching, a MIME type setup block, and the <code>alias</code> directive. You may have noticed that the previous location block used <code>root</code> instead of <code>alias</code>. The two directives are similar, but there is an important difference: <code>alias</code> will use a directory path as-is, while <code>root</code> will append the <code>location</code> path to the one specified in the <code>root</code> directive. For example:</p><pre><code class="language-nginx">location /asdf {
	# Files will be served at {domain}/asdf from the /www/asdf directory
	root /www;
}

location /fdsa {
	# Files will be served at {domain}/fdsa from the /www directory
	alias /www;
}</code></pre><p>Starting on line 55 is the <code>rtmp</code> block. This block comes from the RTMP module and is responsible for nginx&apos;s RTMP and HLS processing. The <code>listen 1935;</code> directive on line 57 tells nginx to listen on port 1935 for RTMP connections. While IANA still <a href="https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=1935">defines this port</a> as &quot;macromedia-fcs&quot;, it is in fact the standard RTMP port.</p><p>The <code>on_publish</code> directive on line 60 is what allows me to perform authentication on incoming RTMP streams. The way this works is, nginx sends a POST to the specified URL with form data containing connection details. If it gets an HTTP 200 response, then it allows the stream to proceed. If it gets an HTTP 401, then it prevents the stream from being received and processed. I wrote a little Python script to let me authenticate streams against LDAP, and it&apos;s been working well.</p><p>The <code>application stream</code> block on line 62 creates an RTMP endpoint named &quot;stream&quot;, such that a tool like OBS will be able to connect to <code>rtmp://streaming.example.com/stream</code>. The specifics of the various directives in this block can be found on <a href="https://github.com/arut/nginx-rtmp-module/wiki/Directives">the module&apos;s GitHub wiki</a>, but there are a few important ones I&apos;d like to point out. First, <code>hls_path /data/hls/livestream;</code> on line 71: as you may have noticed, the path here is the same as the one for the <code>/livestream</code> location up above. This is not a coincidence; you must have a way to access the HLS data being output by the RTMP module via the web.</p><p>The other directives I&apos;d like to mention are <code>hls_fragment 4;</code> and <code>hls_playlist_length 16;</code>. I&apos;ve played around with these values a <em>lot</em> in my quest to decrease the latency between starting a movie and everyone actually seeing it. Unfortunately, when the values are too low, nginx seems to have trouble serving the HLS data, and you end up with half-functioning streams and periods of HTTP 404s for no apparent reason. If you search online, you&apos;ll find lots of different values. Cloudflare, for example, <a href="https://blog.cloudflare.com/building-cloudflare-tv-from-scratch/">uses 1 and 4</a>, respectively. I was not able to get a stable stream with them set that low.</p><h2 id="the-source-side-configuration">The source side configuration</h2><p>I am by no means an expert in using OBS, but a basic configuration is pretty easy. For configuring streaming, simply set the server and key (&quot;my-key&quot; as an example) under the Streaming settings:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2020/09/image-12.png" class="kg-image" alt="Self-hosting a streaming video platform" loading="lazy" width="983" height="758" srcset="https://trycatch.dev/content/images/size/w600/2020/09/image-12.png 600w, https://trycatch.dev/content/images/2020/09/image-12.png 983w" sizes="(min-width: 720px) 720px"></figure><p>After that, add your video source(s), arrange them as needed, and you&apos;re ready to go. Here&apos;s a summary GIF of this process:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2020/09/obs-movie-usage.gif" class="kg-image" alt="Self-hosting a streaming video platform" loading="lazy" width="1390" height="1072"></figure><p>Romero&apos;s cult classic is being played using the VLC Video Source in the above GIF. That source becomes available only when VLC is detected on your machine. If you do not have it installed, the Media Source option should work as well.</p><p>Once streaming begins, the web player should be able to access the HLS stream, in this example, under <code>http://streaming.example.com/livestream/my-key.m3u8</code>.</p><p>If you run into issues, the first thing to do is check the nginx logs and your browser debug console. Between those two, you will likely be able to find any misconfiguration that may have happened. And once everything is working, you can share a virtual movie night experience, too!</p>]]></content:encoded></item><item><title><![CDATA[Self-hosting a video chat system]]></title><description><![CDATA[<p>Since the pandemic began, I&apos;ve been mostly unable to hang out with my friends in the real world. Everyone has switched to various video chat services, such as Zoom and Google Meet. I wanted to see what it would take to host my own video chat service, for</p>]]></description><link>https://trycatch.dev/blog/self-hosting-a-video-chat-system/</link><guid isPermaLink="false">5f5985787562bf0001be8f96</guid><category><![CDATA[Docker]]></category><category><![CDATA[Linux]]></category><category><![CDATA[Self-hosting]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Thu, 10 Sep 2020 21:59:25 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1588196749597-9ff075ee6b5b?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1588196749597-9ff075ee6b5b?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Self-hosting a video chat system"><p>Since the pandemic began, I&apos;ve been mostly unable to hang out with my friends in the real world. Everyone has switched to various video chat services, such as Zoom and Google Meet. I wanted to see what it would take to host my own video chat service, for privacy as well as just general interest in the subject.</p><h2 id="what-choices-are-out-there">What choices are out there?</h2><ul><li><a href="https://github.com/wireapp">Wire</a>: open source, security-focused messenger system</li><li><a href="https://github.com/edumeet/edumeet">eduMeet</a> (and other <a href="https://github.com/versatica/mediasoup">mediasoup</a>-based solutions): open source, web-based video chat system</li><li><a href="https://github.com/ianramzy/decentralized-video-chat">Zipcall</a>: open source, web-based P2P video chat system</li><li><a href="https://github.com/jitsi/jitsi-meet">Jitsi Meet</a>: open source, web-based video chat system</li><li>Probably many others...</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" class="kg-image" alt="Self-hosting a video chat system" loading="lazy" width="5434" height="3623" srcset="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=600&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 600w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1000w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1600&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1600w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2400&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 2400w" sizes="(min-width: 720px) 720px"><figcaption>Photo by <a href="https://unsplash.com/@soymeraki?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Javier Allegue Barros</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption></figure><p>It certainly appears that there is no shortage of possible solutions to choose from, but I had specific hard and soft requirements that ultimately helped me to select one. I wanted a system that was easy to set up, which (at the time anyway) eliminated Wire. I needed to support more than two people in a video call, so Zipcall was not an option. I wanted some integration with an authentication/authorization system, so eduMeet was not a great fit. Thus, Jitsi Meet was chosen.</p><h2 id="the-hosting-situation">The hosting situation</h2><p>I have a Virtual Private Server (VPS) hosted at <a href="https://turnkeyinternet.net/refer/36859">TurnKey Internet</a> <em>(note: affiliate link)</em>. I personally prefer a VPS to cloud providers such as Azure and AWS for multiple reasons. First, there&apos;s general ease of use: I essentially get a virtual machine to do with as I please. There are no complicated billing arrangements, no arcane codenames and barely-comprehensible descriptions for various services, and it&apos;s generally a very streamlined setup. Second, there&apos;s cost. While I can afford cloud hosted solutions, I don&apos;t want to pay more than I have to. Additionally, if I accidentally start some heavy workload on the VPS, the worst that could happen is I get notified that I&apos;m using too many shared resources, whereas I could get charged a significant amount of money on a cloud-hosted solution in a similar situation. Finally, VPSes are often offered by small businesses, rather than giant corporations, so I feel better about using them and supporting such small businesses.</p><p>Getting back on topic, I decided that I wanted to have as much as possible hosted in Docker on my VPS. I didn&apos;t want to set up a complicated environment (certainly no Kubernetes), but I wanted some amount of isolation between the host system and the applications that would live in it, so a plain Docker system seemed like a good idea. To help with managing it, I installed <a href="https://www.portainer.io">Portainer</a>. For accessing the management side of the system securely via SSH as well as the Portainer web interface, I chose to go with a <a href="https://www.zerotier.com/">ZeroTier</a> virtual network, so that I don&apos;t have to expose any unnecessary ports directly to the internet.</p><h2 id="installing-and-configuring-a-jitsi-meet-instance">Installing and configuring a Jitsi Meet instance</h2><p>Thankfully, Jitsi has official support for Docker. It&apos;s almost trivial to get started with the <a href="https://github.com/jitsi/docker-jitsi-meet">docker-jitsi-meet</a> project. Just clone the repo (or download the latest release), customize the <code>.env</code> file, and run <code>docker-compose</code> to bring up the services. More in-depth instructions are available from the linked GitHub repo.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2020/09/image-9.png" class="kg-image" alt="Self-hosting a video chat system" loading="lazy" width="852" height="215" srcset="https://trycatch.dev/content/images/size/w600/2020/09/image-9.png 600w, https://trycatch.dev/content/images/2020/09/image-9.png 852w" sizes="(min-width: 720px) 720px"><figcaption>Portainer&apos;s view of the containers that make up Jitsi Meet</figcaption></figure><p>My changes to the <code>.env</code> file included the usual required ones, such as setting the URL, ports, and the config directory, in addition to enabling LDAP along with guest access: this way, an authorized user must start a meeting, but anyone can join it &#x2014; assuming they know the meeting URL and, optionally, a meeting-specific password. I set the LDAP filter variable such that it would ensure the user is in the Jitsi Meet group before granting access, like so:</p><pre><code class="language-shell">LDAP_FILTER=(&amp;(cn=%u)(memberOf=cn=jitsimeet,ou=groups,dc=example,dc=net))</code></pre><p>Once the Jitsi Meet instance has launched, the config directory will have a number of subdirectories and files under it. To customize the Jitsi web application, edit <code>$CONFIG/web/config.js</code> and <code>$CONFIG/web/interface_config.js</code>. The former file has server-centric configuration items, such as maximum resolution constraints and the ability to enable or disable P2P mode when there are only two users present. The latter file addresses more UI-centric concerns, such as whether to display the Jitsi logo and the layout and size of thumbnails. I modified these based on what I thought worked best, as well as some feedback from people using the system. To see interface changes, you can just reload the Jitsi web page. For the other file, you have to restart the web container. Once you have your instances running and customized, you can start using Jitsi Meet via a web browser or the Jitsi Meet phone apps!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2020/09/image-10.png" class="kg-image" alt="Self-hosting a video chat system" loading="lazy" width="922" height="406" srcset="https://trycatch.dev/content/images/size/w600/2020/09/image-10.png 600w, https://trycatch.dev/content/images/2020/09/image-10.png 922w" sizes="(min-width: 720px) 720px"><figcaption>A Jitsi Meet welcome page</figcaption></figure><h2 id="final-thoughts">Final thoughts</h2><p>Setting up a basic Jitsi Meet instance is pretty simple. I glossed over some details for my specific setup, such as Docker networking shenanigans to let Jitsi containers talk to the LDAP server container and VPS/hosting setup intricacies (firewall configuration, ZeroTier installation and usage), but those are ancillary topics. I could write up more detailed information on those subjects later on if people are interested.</p><p>Up next: self-hosting a video streaming system for virtual movie nights...</p>]]></content:encoded></item><item><title><![CDATA[Doing terrible things with Docker]]></title><description><![CDATA[<p>The Docker philosophy is such that each container should be ephemeral, have a single concern, and contain the absolute minimal amount of data in order to perform its function. I like breaking rules, so I&apos;ve created an image that is antithetical to the Docker philosophy: <a href="https://hub.docker.com/r/arktronic/ubuntu-graphical">arktronic/ubuntu-graphical</a>.</p><h2 id="what-is-it">What</h2>]]></description><link>https://trycatch.dev/blog/doing-terrible-things-with-docker/</link><guid isPermaLink="false">5f567248af1dc10001ff079d</guid><category><![CDATA[Linux]]></category><category><![CDATA[Docker]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Tue, 08 Sep 2020 03:54:17 GMT</pubDate><media:content url="https://trycatch.dev/content/images/2020/09/shocked-docker-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://trycatch.dev/content/images/2020/09/shocked-docker-1.png" alt="Doing terrible things with Docker"><p>The Docker philosophy is such that each container should be ephemeral, have a single concern, and contain the absolute minimal amount of data in order to perform its function. I like breaking rules, so I&apos;ve created an image that is antithetical to the Docker philosophy: <a href="https://hub.docker.com/r/arktronic/ubuntu-graphical">arktronic/ubuntu-graphical</a>.</p><h2 id="what-is-it">What is it?</h2><p>It&apos;s a Docker container image that creates a minimal graphical Linux environment, accessible via SSH and RDP (Microsoft Remote Desktop). Running it produces a system based on Ubuntu, which you can customize: modify any configurations, install any applications, and generally do anything. Out of the box, it comes with the LXDE UI and Firefox. That&apos;s it. Using <code>apt</code> you can install anything else available for Ubuntu. To make the container easy to work with, <code>xrdp</code> is pre-configured to accept RDP connections on its standard port, 3389. As of this writing, version 2.0 also supports audio over RDP, but it&apos;s a bit buggy. You can, in fact, watch YouTube inside the container, if you so desire!</p><h2 id="why">Why?</h2><p>The short answer is, because I wanted to experiment.</p><p>The slightly longer answer is, I wanted to see if I could create a minimal desktop environment that could conceivably be used as a base for a software development container or perhaps a basic Linux system where people could try out a potentially destructive experiment (<code>rm -rf /</code>, anyone?) without expensive consequences.</p><p>Containers fascinate me, but not because of the usual DevOps reasons. Rather, I&apos;m interested in what they can be used for, outside of application hosting. If I could instantly launch a container with a full-blown development environment or a graphics editing system or a DAW with various plugins already pre-configured, what opportunities could this open? This is what I&apos;m interested in exploring.</p><h2 id="what-s-so-terrible-about-this">What&apos;s so terrible about this?</h2><p>All right, it may not be <em>terrible</em> per se, but as mentioned in the introduction, it&apos;s very antithetical to the Docker philosophy. Let&apos;s go over that in a bit more detail.</p><h3 id="containers-should-be-ephemeral">Containers should be ephemeral</h3><p>It should be very easy and cheap to create and destroy a container, so it shouldn&apos;t have much (or any) state. This one, however, encourages customization of the environment, which makes it extremely stateful.</p><h3 id="containers-should-have-one-concern">Containers should have one concern</h3><p>Docker strongly encourages running multiple containers in concert with each other, to build up a system from its individual parts. This container instead has everything, including the kitchen sink (well, there&apos;s an xrdp sink anyway), built right into it. While it could be argued that the one concern here is a working Linux environment, I wouldn&apos;t buy that: it&apos;s like saying, Amazon&apos;s shopping ecosystem is a single concern. That&apos;s too high level to be of any use.</p><h3 id="containers-should-be-minimal">Containers should be minimal</h3><p>In order to facilitate quick and easy creation and destruction of containers, they should only include what is absolutely necessary for them to run. Optional components should not be added in the first place or be removed as part of the image building process. Obviously, since this container is intended to be used by a human, rather than an application, it has to have a bunch of optional components to make it easier to use.</p><h2 id="okay-then-who-s-it-for">Okay then, who&apos;s it for?</h2><p>It&apos;s for anyone interested in exploring Linux or expanding the possibilities of containers. But really, it&apos;s mostly for people to play around.</p><p>You can get it here: <a href="https://hub.docker.com/r/arktronic/ubuntu-graphical">https://hub.docker.com/r/arktronic/ubuntu-graphical</a>, and the GitHub repo is over here: <a href="https://github.com/arktronic/docker-ubuntu-graphical">https://github.com/arktronic/docker-ubuntu-graphical</a></p><p>P.S. I apologize for doing this to the Docker logo:</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2020/09/shocked-docker-2.png" class="kg-image" alt="Doing terrible things with Docker" loading="lazy" width="1316" height="785" srcset="https://trycatch.dev/content/images/size/w600/2020/09/shocked-docker-2.png 600w, https://trycatch.dev/content/images/size/w1000/2020/09/shocked-docker-2.png 1000w, https://trycatch.dev/content/images/2020/09/shocked-docker-2.png 1316w" sizes="(min-width: 720px) 720px"></figure>]]></content:encoded></item><item><title><![CDATA[Fixing Windows moving and resizing your windows after sleep]]></title><description><![CDATA[<p>I just replaced my home dual monitor setup with two newer, 75Hz monitors (that, incidentally, aren&apos;t actively dying on me). Unfortunately, after replacing them, I discovered that all of my windows would move to the primary monitor and occasionally resize when my computer entered sleep mode. This is,</p>]]></description><link>https://trycatch.dev/blog/fixing-windows-moving-and-resizing-your-windows-after-sleep/</link><guid isPermaLink="false">5f52ff4eaf1dc10001ff0685</guid><category><![CDATA[Windows]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Sat, 05 Sep 2020 17:25:55 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1534972195531-d756b9bfa9f2?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1534972195531-d756b9bfa9f2?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Fixing Windows moving and resizing your windows after sleep"><p>I just replaced my home dual monitor setup with two newer, 75Hz monitors (that, incidentally, aren&apos;t actively dying on me). Unfortunately, after replacing them, I discovered that all of my windows would move to the primary monitor and occasionally resize when my computer entered sleep mode. This is, of course, unacceptable.</p><p>Searching online yielded many interesting results, from Registry tweaks to updating drivers to putting tape on certain HDMI pins in order to prevent monitors entering deep sleep. Most of the &quot;fixes&quot; were very sketchy with lots of people saying they didn&apos;t do anything, and I didn&apos;t want to resort to messing with my connectors until I&apos;d exhausted other avenues.</p><p>By chance, I was scrolling through <a href="https://answers.microsoft.com/en-us/windows/forum/windows_10-hardware/windows-10-multiple-display-windows-are-moved-and/2b9d5a18-45cc-4c50-b16e-fd95dbf27ff3">yet another thread</a> of tweaks when I stumbled upon an important part of the answer by user &quot;st99&quot;: Windows remembers your old monitors! When you disconnect a monitor and connect another one, or sometimes install a different driver, Windows will see your monitor as a different device, but it will still keep track of the previous device. As your PC goes to sleep and monitors are seen as disconnected, Windows doesn&apos;t know which monitors are valid any more and just chooses one &#x2014; possibly the first one registered. Because multiple monitors are no longer detected, all windows are moved to the one &quot;active&quot; monitor, and sometimes resized if that monitor&apos;s resolution is too low.</p><p>In order to avoid this issue, you need to remove all the monitors that no longer exist.</p><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2020/09/image-4.png" class="kg-image" alt="Fixing Windows moving and resizing your windows after sleep" loading="lazy" width="724" height="555" srcset="https://trycatch.dev/content/images/size/w600/2020/09/image-4.png 600w, https://trycatch.dev/content/images/2020/09/image-4.png 724w" sizes="(min-width: 720px) 720px"></figure><p>Instructions:</p><ol><li>Open the Device Manager by right-clicking the Start button and selecting Device Manager</li><li>Select the <em>View</em> menu item, and click <em>Show hidden devices</em> (see above screenshot)</li><li>Expand the <em>Monitors</em> section</li><li>Find all entries inside the <em>Monitors</em> section that have a grayed out or light-colored icon and delete them (in the screenshot above, that would be the first two &quot;CB272U&quot; entries)</li></ol><p>The other part of the solution to this problem is to avoid DisplayPort. For whatever reason, certain DisplayPort-connected monitors continue to cause Windows to exhibit the unwanted window moving/resizing behavior. If you&apos;ve deleted all the disconnected monitors and the issue still occurs, try switching from DP to HDMI or DVI.</p><p>This issue was encountered on Windows 10 version 2004, but it has apparently existed since Windows 7 or even earlier. Microsoft really should fix it by having a more intelligent way of remembering what monitors were connected and configured last.</p><h3 id="tl-dr">tl;dr</h3><p>Delete old monitor entries from Device Manager, and avoid DisplayPort.</p>]]></content:encoded></item><item><title><![CDATA[Raspberry Pi-powered webcam]]></title><description><![CDATA[<p>With the pandemic continuing to force many people, including myself, to work from home, effective communication tools have become paramount for day-to-day teamwork. Video chat in particular has caused popular models of webcams to be sold out for weeks or months. While I was lucky to get my hands on</p>]]></description><link>https://trycatch.dev/blog/raspberry-pi-powered-webcam/</link><guid isPermaLink="false">5f4adc199791400001e5fc03</guid><category><![CDATA[Python]]></category><category><![CDATA[Raspberry Pi]]></category><category><![CDATA[3D Printing]]></category><category><![CDATA[3D Modeling]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Sun, 30 Aug 2020 16:51:22 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1587919057555-d728ff5beac3?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1587919057555-d728ff5beac3?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Raspberry Pi-powered webcam"><p>With the pandemic continuing to force many people, including myself, to work from home, effective communication tools have become paramount for day-to-day teamwork. Video chat in particular has caused popular models of webcams to be sold out for weeks or months. While I was lucky to get my hands on a <a href="https://www.logitech.com/en-us/product/c922-pro-stream-webcam">Logitech C922</a> pretty early on, I wanted to experiment with other potential solutions, especially ones that let me control zoom and focus more precisely.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2020/08/image.png" class="kg-image" alt="Raspberry Pi-powered webcam" loading="lazy" width="600" height="400" srcset="https://trycatch.dev/content/images/2020/08/image.png 600w"><figcaption>Image courtesy of the Raspberry Pi Foundation</figcaption></figure><p>This led me to the <a href="https://www.raspberrypi.org/products/raspberry-pi-high-quality-camera/">Raspberry Pi High Quality Camera</a>. It&apos;s a camera module, similar to the other Pi cameras that have been available for years now, but with a 12.3MP Sony sensor and support for C- and CS-mount compatible lenses. It also features a standard 1/4&quot;-20 UNC tripod mount.</p><h2 id="hardware-and-software">Hardware and software</h2><p>I&apos;ve got quite a few Raspberry Pi computers laying around, from the very first release (with a rather fragile full-size SD card slot) to the latest Raspberry Pi 4 varieties, so finding one to use wasn&apos;t an issue for me. I purchased an HQ Camera module and a few random C- and CS-mount lenses. Unfortunately, the fixed focal length prime lenses that I initially got didn&apos;t work well for me, as their optimal distance was outside of my setup. I eventually bought a <a href="https://www.ebay.com/itm/2-8mm-12mm-Manual-Zoom-Manual-Focal-CS-Mount-Lens-for-CCTV-Cameras/272293277356">zoom lens</a> with manual focus, which has done its job pretty well so far.</p><p>Next, I had to figure out a good way to get the camera stream into my desktop computer. The <code><a href="https://www.raspberrypi.org/documentation/usage/camera/raspicam/raspivid.md">raspivid</a></code> application that comes with Raspberry Pi OS has the ability to stream video data over a network directly or via a pipe to another app. My first attempt involved using <code>netcat</code> to stream to my Windows PC, receiving the stream with <code><a href="https://nmap.org/ncat/">ncat</a></code>, and piping the data into MPlayer. It mostly worked, but the stream was laggy and had occasional hiccups, although those were probably caused by the Pi&apos;s wireless connection. This didn&apos;t seem like a good solution. Even if a more stable network connection solved both the latency and hiccup issues, there was still the problem of getting MPlayer (or something else) to act as a webcam. The closest I came to getting that to work was using <a href="https://obsproject.com/">OBS</a> to capture the MPlayer window and export that stream as a <a href="https://obsproject.com/forum/resources/obs-virtualcam.949/">virtual webcam</a>, but this was rapidly becoming an extremely hacky solution. There had to be a better alternative.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2020/08/image-1.png" class="kg-image" alt="Raspberry Pi-powered webcam" loading="lazy" width="1600" height="1600" srcset="https://trycatch.dev/content/images/size/w600/2020/08/image-1.png 600w, https://trycatch.dev/content/images/size/w1000/2020/08/image-1.png 1000w, https://trycatch.dev/content/images/2020/08/image-1.png 1600w" sizes="(min-width: 720px) 720px"><figcaption>A capture card on <a href="https://www.ebay.com/itm/122827929117">eBay</a></figcaption></figure><p>Enter: USB capture cards! An entire industry has sprung up around video game live streaming, play-throughs, speedruns, and similar forms of entertainment showing gameplay. Capture cards are used to perform the actual capturing and relaying of video and audio data from game consoles, as well as other sources, to PCs for streaming or recording. <em>(The term &quot;capture card&quot; is a little misleading today. Historically referring to peripherals connected to motherboard expansion slots, its definition has now grown to include external USB peripherals as well.)</em></p><p>The specific capture card that I purchased happens to be widely available under various names on <a href="https://www.ebay.com/itm/122827929117">eBay</a> and <a href="https://smile.amazon.com/dp/B08CMXLN88">Amazon</a>. This particular model&apos;s feature listing is a little misleading, as it suggests that the device can record 4K streams. Well, it can <em>capture</em> 4K streams, and it can pass them through to the output HDMI port, but it can only output a maximum resolution of 1080p over USB. Still, that&apos;s pretty good functionality for the price, and I don&apos;t need 4K for what I&apos;m doing anyway. As you might have guessed, I&apos;m connecting the Raspberry Pi to the capture card and using that as a webcam. In fact, with this model anyway, the setup was a lot simpler than I anticipated. Instead of showing up in Device Manager as a custom USB peripheral requiring weird third-party drivers, it appears as a webcam! The only thing left to do is configure the Raspberry Pi to output the camera&apos;s video stream to the main screen.</p><p>I chose a simple Pi configuration, at least for now. I might write something more advanced that allows live tweaking of camera settings later on, but at the moment, built-in applications are sufficient. I used Raspberry Pi OS Lite for speed. Since the camera preview can be rendered directly onto the Linux framebuffer, there was no need to install a GUI at all. A simple script was used to launch the app:</p><figure class="kg-card kg-code-card"><pre><code class="language-shell">#!/bin/bash
TERM=xterm clear &gt; /dev/tty1
raspivid -t 0 -drc med -sa 25</code></pre><figcaption>/root/service-script.sh</figcaption></figure><p>The first command, after the <a href="https://en.wikipedia.org/wiki/Shebang_(Unix)">shebang line</a>, clears the main screen&apos;s terminal, which may still have output from the boot sequence. The second line then launches <code>raspivid</code>, which shows the video preview.</p><p>To get this script to run at startup, I wrote a basic systemd service:</p><figure class="kg-card kg-code-card"><pre><code class="language-ini">[Unit]
Description=Pi Camera Service

[Service]
Type=simple
ExecStart=/bin/bash /root/service-script.sh

[Install]
WantedBy=multi-user.target</code></pre><figcaption>/root/camera.service</figcaption></figure><p>To install the systemd service, I did the following:</p><figure class="kg-card kg-code-card"><pre><code class="language-shell">ln -s /root/camera.service /etc/systemd/system/
systemctl daemon-reload
systemctl enable camera</code></pre><figcaption>Execute these commands as root</figcaption></figure><p>The <code>enable</code> command makes the service run automatically at startup. To launch it immediately, execute <code>systemctl start camera</code>.</p><p>Finally, a couple of tweaks had to be done to the boot config and kernel command-line files:</p><figure class="kg-card kg-code-card"><pre><code class="language-ini">hdmi_group=1
hdmi_mode=16</code></pre><figcaption>Excerpt from /boot/config.txt</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-none">console=serial0,115200 console=tty3 root=PARTUUID=b48c1aa6-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait logo.nologo vt.global_cursor_default=0</code></pre><figcaption>/boot/cmdline.txt</figcaption></figure><p>The <code>config.txt</code> file was modified to force 1080p, and the <code>cmdline.txt</code> file was modified to hide additional text output, the Pi logos, and the blinking cursor.</p><h2 id="mounting">Mounting</h2><p>To get everything situated on my monitor, I decided to design and 3D print my own mount:</p><figure class="kg-card kg-code-card"><pre><code class="language-clike">// large vertical piece with thread
difference() {
linear_extrude(height = 74)
            square([80, 4]);

translate([40, -3, 4])
    rotate([270, 0, 0])
        english_thread (diameter=1/4, threads_per_inch=20, length=1, internal=true);

rpi_holes(5, 14);
}

// rear horizontal piece with thread
difference() {
translate([0, 0, 70])
    linear_extrude(height = 4)
        square([80, 60]);

translate([40, 38, 60])
    english_thread (diameter=1/4, threads_per_inch=20, length=1, internal=true);
}

// front horizontal piece
translate([0, -35, 70])
    linear_extrude(height = 4)
        square([80, 35]);

// front overhang piece
translate([0, -35, 64])
    linear_extrude(height = 10)
        square([80, 3]);



module hole(x, y) {
    color(&quot;red&quot;)
        rotate([90, 0, 0])
            translate([x, y, 0])
                cylinder(d=2.7, h=15, $fn=100, center=true);
}

// WARNING: these may be inaccurate!
module rpi_holes(x, y) {
    hole(x, y);
    hole(x + 58, y);
    hole(x, y + 49);
    hole(x + 58, y + 49);
}</code></pre><figcaption>OpenSCAD code for the mount</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2020/08/image-2.png" class="kg-image" alt="Raspberry Pi-powered webcam" loading="lazy" width="945" height="842" srcset="https://trycatch.dev/content/images/size/w600/2020/08/image-2.png 600w, https://trycatch.dev/content/images/2020/08/image-2.png 945w" sizes="(min-width: 720px) 720px"><figcaption>OpenSCAD was used to design the mount</figcaption></figure><p>I used <a href="https://www.openscad.org/">OpenSCAD</a> to model the mount and then export the model as a 3MF file for slicing with Simplify3D. The <code>english_thread</code> function came from <a href="https://www.dkprojects.net/openscad-threads/">Dan Kirshner</a>. In addition to the tripod mount holes, I also printed the nuts and bolts to go along with them. Creating the bolt was straightforward:</p><figure class="kg-card kg-code-card"><pre><code class="language-clike">cylinder(r=8, h=6, $fn=6);
english_thread (diameter=1/4, threads_per_inch=20, length=1.5);</code></pre><figcaption>OpenSCAD code for a 1/4&quot;-20 bolt</figcaption></figure><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2020/08/image-5.png" class="kg-image" alt="Raspberry Pi-powered webcam" loading="lazy" width="275" height="622"></figure><p>Creating a nut was also pretty clear:</p><figure class="kg-card kg-code-card"><pre><code class="language-clike">difference()
{
    cylinder(r=8, h=6, $fn=6);
    translate([0, 0, -0.1])
        english_thread (diameter=1/4, threads_per_inch=20, length=0.5, internal=true);
}</code></pre><figcaption>OpenSCAD code for a 1/4&quot;-20 nut</figcaption></figure><figure class="kg-card kg-image-card"><img src="https://trycatch.dev/content/images/2020/08/image-4.png" class="kg-image" alt="Raspberry Pi-powered webcam" loading="lazy" width="583" height="556"></figure><p>This didn&apos;t <em>exactly</em> turn out as I imagined, but it got the job done:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://trycatch.dev/content/images/2020/08/image-3.png" class="kg-image" alt="Raspberry Pi-powered webcam" loading="lazy" width="2000" height="1482" srcset="https://trycatch.dev/content/images/size/w600/2020/08/image-3.png 600w, https://trycatch.dev/content/images/size/w1000/2020/08/image-3.png 1000w, https://trycatch.dev/content/images/size/w1600/2020/08/image-3.png 1600w, https://trycatch.dev/content/images/size/w2400/2020/08/image-3.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Two mounts in use</figcaption></figure><p>The 1/4&quot;-20 threading on the mount and on the printed nuts and bolts worked surprisingly well for both mounting the HQ camera (the top bolt) and for changing the camera&apos;s pitch (the bottom bolt). Since everything is made of PLA, there is no concern of damaging the monitor or the camera module, and the plastic is just malleable enough that I was able to use an existing tripod with metal threading as a makeshift thread chaser to ensure the printed threading was correct.</p><p>Due to the structure of my monitor, I was unable to use a single mount for both the camera and the Raspberry Pi: the HDMI connection location made that impossible &#x2014; see the awkward angle of the Pi in the above photo. Also, I realized that I had no M2.5 screws, so I just used some twist ties to secure the Raspberry Pi to the second mount. That had the nice side effect of letting me avoid dealing with a potentially inaccurate calculation for the locations of the Raspberry Pi mount holes. <em>(The <a href="https://www.raspberrypi.org/documentation/hardware/raspberrypi/mechanical/rpi_MECH_4b_4p0.pdf">mechanical drawing</a> clearly indicates the distances between the centers of the mount holes, which is the information I used, but, after printing, the holes appear to be just slightly misaligned. I&apos;m uncertain as to the cause.)</em></p><p>Overall, this was a fun project! Between designing and printing my own monitor mount and learning something about camera lenses (I still have a lot to learn in that area), I&apos;ve got new experiences and useful knowledge for future projects. If you have any questions or suggestions, <a href="https://twitter.com/arktronic">hit me up on Twitter</a>.</p><h2 id="p-s-possible-future-enhancements">P.S.: Possible future enhancements</h2><p>A few things I might do later on to make all of this work better:</p><ul><li>Replace the shell script for launching <code>raspivid</code> with a Python script utilizing the <code><a href="https://picamera.readthedocs.io/">picamera</a></code> package, so that I can adjust camera parameters in real time, possibly via a simple web interface</li><li>Fix the mounting holes in an updated design and use a single mount once my new monitors arrive (their design does not have the protrusion seen in the above photo)</li><li>Connect my microphone to the Raspberry Pi so that audio and video would both come from the same source</li></ul>]]></content:encoded></item><item><title><![CDATA[Boo! A static Ghost!]]></title><description><![CDATA[<p>I&apos;ve been looking to restart blogging for a while now. A number of things held me back, not the least of which is blog technology. I&apos;ve written a couple iterations of my own .NET-based static site generator, but it just wasn&apos;t very exciting for</p>]]></description><link>https://trycatch.dev/blog/boo-a-static-ghost/</link><guid isPermaLink="false">5f45a472f21c2c0001b5a7a4</guid><category><![CDATA[Meta]]></category><category><![CDATA[Python]]></category><category><![CDATA[GitHub Pages]]></category><dc:creator><![CDATA[Sasha]]></dc:creator><pubDate>Sat, 29 Aug 2020 00:40:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1476283721796-dd935b062838?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1476283721796-dd935b062838?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Boo! A static Ghost!"><p>I&apos;ve been looking to restart blogging for a while now. A number of things held me back, not the least of which is blog technology. I&apos;ve written a couple iterations of my own .NET-based static site generator, but it just wasn&apos;t very exciting for me. I think I&apos;ve finally figured out why.</p><h2 id="static-site-generators">Static site generators</h2><p>The standard static site generator uses config and markup (well, Markdown, usually) files in various locations on a filesystem, in conjunction with themed templates, to generate websites that then get hosted on a basic web server without an interactive backend. I <em>love</em> that from a security perspective: that&apos;s a whole class of possible vulnerabilities gone. But I&apos;ve only recently realized that I don&apos;t enjoy it from a UX perspective.</p><p>As a techie, and software developer, I live and breathe the filesystem, config files, and command-line tools. So, it came as a surprise to me when I made the realization that I don&apos;t want to work with these things (that I normally hold dear) when writing blog posts. I think there are two major reasons for this: perfectionism and overall mindset.</p><p>I am often a perfectionist to a fault. Not to digress too far, but this trait has caused me a lot of trouble, especially when it comes to working on &#x2014; and completing &#x2014; personal projects. How does that manifest when it comes to using static site generators for blogging? To put it simply, I don&apos;t think any of them are good enough. Each one has its own limitations, which I invariably encounter, and that severely detracts from my enjoyment of the tools. This, of course, includes my own creations. As I attempt to fix the issues I see with other generators, I introduce new issues and complexities into my own solutions. It&apos;s become a never-ending cycle of annoyance and rework. And in the meantime, I&apos;m not blogging. I&apos;m just working on code that would theoretically, eventually enable me to do so.</p><p>The other aspect of UX I found that I miss is the old WYSIWYG editor. Or even a basic rich text editor. Static site generators, by their very nature, don&apos;t come with anything like that. You can instead use whatever editor application you prefer. Lots of editors let you preview Markdown text as you type it, in fact. That&apos;s great, but I&apos;ve discovered that I go into a different mindset when I write English rather than code, and mixing the two will often make me lose sight of what I&apos;m saying and focus too much on the technical aspects of getting it displayed <em>just right</em>.</p><h2 id="traditional-platforms">Traditional platforms</h2><p>On the other side of the blogging technology spectrum are the &quot;traditional&quot; blogging platforms, content management systems (CMS), and such. While these do provide a much better UX, they are less secure than static sites due to the fact that they do backend processing, so the attack surface is larger. With that concern comes the requirement to keep the platform hardened and up to date, or to pay someone else to do it.</p><p>In addition to the increased security concerns, the traditional platforms are naturally slower, again due to the fact that they do <em>any</em> backend processing. As optimized as they may get, it would still be hard to beat a simple web server hosting static files.</p><p>I could pay for blog hosting. I probably wouldn&apos;t notice any appreciable difference in performance between a true static site and a CMS-backed one. But I want to be in control of my data, and there&apos;s still the nagging feeling I get of wanting to implement a customized solution just for me &#x2014; not some bog standard <em>cough</em> Medium <em>cough</em> thing that everyone else does.</p><!--kg-card-begin: markdown--><p>I could pay for blog hosting. I probably wouldn&apos;t notice any appreciable difference in performance between a true static site and a CMS-backed one. But I want to be in control of my data, and there&apos;s still the nagging feeling I get of wanting to implement a customized solution just for me &#x2014; not some bog standard <small><em>cough</em> Medium <em>cough</em></small> thing that everyone else does.</p>
<!--kg-card-end: markdown--><p>As I mentioned above, blogging technologies lie on a spectrum. What other options are out there that lie between the two extremes?</p><h2 id="headless-and-hybrid-cms">Headless and hybrid CMS</h2><p>The concept of a headless CMS has been gaining popularity of late. In essence, a headless CMS is one that has a full-featured backend infrastructure, along with the associated content creation and management user interfaces, but has no frontend (&quot;head&quot;) and instead provides an API to retrieve the content for displaying in another system. That term, along with &quot;hybrid&quot; when applied to a CMS, is not well defined today, so there is some variability in what different vendors call &quot;headless&quot;. For the purposes of this post, let&apos;s say that &quot;headless&quot; means no frontend whatsoever, and &quot;hybrid&quot; means there is a default frontend in addition to an API.</p><p>The <a href="https://github.com/TryGhost/Ghost">Ghost</a> platform falls into the hybrid category. It has a frontend, with theming support, but it also has a content API. There are static site generator integrations available, some supported by Ghost directly. When I got this far in my research, I was quite excited, since it appeared that I&apos;d found the solution I was looking for. Alas, I ran into a couple of issues with this approach that prevented me from choosing it. The first one was that none of the themes I found worked nearly as well as the built in and freely available ones for Ghost. That&apos;s not overall a big deal, of course, but I had already started an instance of Ghost in a Docker container and was starting to appreciate its theming, so when I saw the difference between how posts look in Ghost versus how they look rendered in other systems with multiple different themes coming nowhere close to the polish of default Ghost themes, I was pretty disappointed. Even the ported versions of the default &quot;Casper&quot; theme weren&apos;t working as well outside of Ghost.</p><p>The other issue was that I would now have to configure and maintain two independent systems to allow me to blog the way I want. And I&apos;ve witnessed far too many instances of breaking API changes and other integration problems with different applications that this isn&apos;t something I felt I&apos;d enjoy maintaining.</p><p>Instead, I decided to try a different approach: scraping!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://images.unsplash.com/photo-1567025557402-9aab2c0385d5?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" class="kg-image" alt="Boo! A static Ghost!" loading="lazy" width="4898" height="3265" srcset="https://images.unsplash.com/photo-1567025557402-9aab2c0385d5?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=600&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 600w, https://images.unsplash.com/photo-1567025557402-9aab2c0385d5?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1000w, https://images.unsplash.com/photo-1567025557402-9aab2c0385d5?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1600&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1600w, https://images.unsplash.com/photo-1567025557402-9aab2c0385d5?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2400&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 2400w" sizes="(min-width: 720px) 720px"><figcaption>Photo by <a href="https://unsplash.com/@gxjansen?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Guido Jansen</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption></figure><p>Yeah, I know, this isn&apos;t the early 2000s, and I&apos;m not a search aggregator. But scraping actually makes sense in this instance:</p><ul><li>I have control over the source system;</li><li>Being a hybrid CMS, Ghost does minimal backend processing to display content;</li><li>Themes should work just fine after being scraped.</li></ul><p>Turns out, there&apos;s already a project that does something like this specifically for <a href="https://github.com/axitkhurana/buster">Ghost: buster</a>. Unfortunately, it&apos;s unmaintained and doesn&apos;t actually do a proper job scraping because under the hood it uses <code>wget</code>, which makes some unfortunate changes to the retrieved contents, such as indiscriminately turning all same-site absolute URLs into relative ones. I couldn&apos;t come up with an obvious way to fix that tool.</p><p>That means, of course, I wrote my own. Keeping with the theme, I called it <a href="https://github.com/arktronic/ecto1">ecto1</a>. It&apos;s a Python 3 script, utilizing BeautifulSoup and tinycss2 for parsing content for further URLs to fetch. Pointing to Ghost&apos;s Sitemap file reveals all the content URLs, while those content URLs themselves point to resources such as images and scripts. Since essentially everything is downloaded, Ghost themes work just fine: after all, they&apos;re just HTML and associated resources.</p><p>To complete the workflow, I&apos;m using GitHub Pages for hosting. I intended to use GitHub Actions along with Ghost&apos;s built-in webhook support, but there doesn&apos;t appear to be any way to trigger a GitHub Action with the built-in integrations, so I&apos;ve got a simple script running together with Ghost that gets triggered to execute ecto1. I&apos;m going to experiment with this setup for a little while to see if I like it. Meanwhile, <code>hello again, world</code>!</p>]]></content:encoded></item></channel></rss>